{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text author\n",
      "0  This process, however, afforded me no means of...    EAP\n",
      "1  It never once occurred to me that the fumbling...    HPL\n",
      "2  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  Finding nothing else, not even gold, the Super...    HPL\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "df = df.drop(['id'], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'EAP': 0, u'HPL': 1, u'MWS': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_mapping = {label:idx for idx,label in enumerate(np.unique(df['author']))}\n",
    "author_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  author\n",
      "0  This process, however, afforded me no means of...       0\n",
      "1  It never once occurred to me that the fumbling...       1\n",
      "2  In his left hand was a gold snuff box, from wh...       0\n",
      "3  How lovely is spring As we looked from Windsor...       2\n",
      "4  Finding nothing else, not even gold, the Super...       1\n"
     ]
    }
   ],
   "source": [
    "df['author'] = df['author'].map(author_mapping)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:18\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing the data:\n",
    "## Separate words and \n",
    "## count each word's occurrence\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['text']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['text']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'text'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u',', u'the', u'of', u'.', u'and']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "## Create a mapping:\n",
    "## Map each unique word to an integer\n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['text']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['text']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define fixed-length sequences:\n",
    "## Use the last 200 elements of each sequence\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "\n",
    "sequence_length = 300  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "X_train = sequences[:15500, :]\n",
    "y_train = df.loc[:15500, 'author'].values\n",
    "X_test = sequences[15500:, :]\n",
    "y_test = df.loc[15500:, 'author'].values\n",
    "\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size   ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                    shape=(self.batch_size, self.seq_len),\n",
    "                    name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                    shape=(self.batch_size),\n",
    "                    name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                    name='tf_keepprob')\n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "\n",
    "        ## Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                 self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                 cells, embed_x,\n",
    "                 initial_state=self.initial_state)\n",
    "        ## Note: lstm_outputs shape: \n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        ## Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "                 inputs=lstm_outputs[:, -1],\n",
    "                 units=1, activation=None,\n",
    "                 name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                 tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                 labels=tf_y, logits=logits),\n",
    "                 name='cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                             self.final_state],\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/spooky-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('  << initial state >> ', (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),))\n",
      "('\\n  << lstm_output   >> ', <tf.Tensor 'rnn/transpose:0' shape=(100, 300, 128) dtype=float32>)\n",
      "('\\n  << final state   >> ', (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>),))\n",
      "('\\n  << logits        >> ', <tf.Tensor 'logits_squeezed:0' shape=(100,) dtype=float32>)\n",
      "('\\n  << predictions   >> ', {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>})\n"
     ]
    }
   ],
   "source": [
    "## Train:\n",
    "\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, \n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256, \n",
    "                   lstm_size=128, \n",
    "                   num_layers=1, \n",
    "                   batch_size=100, \n",
    "                   learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 Iteration: 20 | Train loss: 0.69409\n",
      "Epoch: 1/200 Iteration: 40 | Train loss: -0.82173\n",
      "Epoch: 1/200 Iteration: 60 | Train loss: 1.42024\n",
      "Epoch: 1/200 Iteration: 80 | Train loss: 0.14882\n",
      "Epoch: 1/200 Iteration: 100 | Train loss: -0.23103\n",
      "Epoch: 1/200 Iteration: 120 | Train loss: -0.44219\n",
      "Epoch: 1/200 Iteration: 140 | Train loss: -0.88181\n",
      "Epoch: 2/200 Iteration: 160 | Train loss: -2.65125\n",
      "Epoch: 2/200 Iteration: 180 | Train loss: -1.74752\n",
      "Epoch: 2/200 Iteration: 200 | Train loss: -1.28096\n",
      "Epoch: 2/200 Iteration: 220 | Train loss: -0.95793\n",
      "Epoch: 2/200 Iteration: 240 | Train loss: -1.11271\n",
      "Epoch: 2/200 Iteration: 260 | Train loss: -2.00011\n",
      "Epoch: 2/200 Iteration: 280 | Train loss: -3.62694\n",
      "Epoch: 2/200 Iteration: 300 | Train loss: -4.84606\n",
      "Epoch: 3/200 Iteration: 320 | Train loss: -8.56320\n",
      "Epoch: 3/200 Iteration: 340 | Train loss: -9.22080\n",
      "Epoch: 3/200 Iteration: 360 | Train loss: -9.36788\n",
      "Epoch: 3/200 Iteration: 380 | Train loss: -5.81568\n",
      "Epoch: 3/200 Iteration: 400 | Train loss: 0.40129\n",
      "Epoch: 3/200 Iteration: 420 | Train loss: -4.71822\n",
      "Epoch: 3/200 Iteration: 440 | Train loss: -8.19315\n",
      "Epoch: 3/200 Iteration: 460 | Train loss: -11.80715\n",
      "Epoch: 4/200 Iteration: 480 | Train loss: -10.74283\n",
      "Epoch: 4/200 Iteration: 500 | Train loss: -14.60092\n",
      "Epoch: 4/200 Iteration: 520 | Train loss: -15.93577\n",
      "Epoch: 4/200 Iteration: 540 | Train loss: -8.99684\n",
      "Epoch: 4/200 Iteration: 560 | Train loss: -9.21649\n",
      "Epoch: 4/200 Iteration: 580 | Train loss: -12.22167\n",
      "Epoch: 4/200 Iteration: 600 | Train loss: -15.34551\n",
      "Epoch: 4/200 Iteration: 620 | Train loss: -13.41837\n",
      "Epoch: 5/200 Iteration: 640 | Train loss: -9.38122\n",
      "Epoch: 5/200 Iteration: 660 | Train loss: -26.25293\n",
      "Epoch: 5/200 Iteration: 680 | Train loss: -15.14328\n",
      "Epoch: 5/200 Iteration: 700 | Train loss: -15.37042\n",
      "Epoch: 5/200 Iteration: 720 | Train loss: -24.14262\n",
      "Epoch: 5/200 Iteration: 740 | Train loss: -19.72305\n",
      "Epoch: 5/200 Iteration: 760 | Train loss: -34.45713\n",
      "Epoch: 6/200 Iteration: 780 | Train loss: -30.55996\n",
      "Epoch: 6/200 Iteration: 800 | Train loss: -25.08046\n",
      "Epoch: 6/200 Iteration: 820 | Train loss: -19.85628\n",
      "Epoch: 6/200 Iteration: 840 | Train loss: -21.67873\n",
      "Epoch: 6/200 Iteration: 860 | Train loss: -31.11999\n",
      "Epoch: 6/200 Iteration: 880 | Train loss: -17.73773\n",
      "Epoch: 6/200 Iteration: 900 | Train loss: -29.35684\n",
      "Epoch: 6/200 Iteration: 920 | Train loss: -30.69534\n",
      "Epoch: 7/200 Iteration: 940 | Train loss: -37.76020\n",
      "Epoch: 7/200 Iteration: 960 | Train loss: -43.76160\n",
      "Epoch: 7/200 Iteration: 980 | Train loss: -43.37392\n",
      "Epoch: 7/200 Iteration: 1000 | Train loss: -29.79609\n",
      "Epoch: 7/200 Iteration: 1020 | Train loss: -21.26038\n",
      "Epoch: 7/200 Iteration: 1040 | Train loss: -31.41461\n",
      "Epoch: 7/200 Iteration: 1060 | Train loss: -38.78999\n",
      "Epoch: 7/200 Iteration: 1080 | Train loss: -46.62306\n",
      "Epoch: 8/200 Iteration: 1100 | Train loss: -38.37183\n",
      "Epoch: 8/200 Iteration: 1120 | Train loss: -46.37939\n",
      "Epoch: 8/200 Iteration: 1140 | Train loss: -47.44137\n",
      "Epoch: 8/200 Iteration: 1160 | Train loss: -35.29472\n",
      "Epoch: 8/200 Iteration: 1180 | Train loss: -38.91565\n",
      "Epoch: 8/200 Iteration: 1200 | Train loss: -45.49755\n",
      "Epoch: 8/200 Iteration: 1220 | Train loss: -43.42214\n",
      "Epoch: 8/200 Iteration: 1240 | Train loss: -43.28991\n",
      "Epoch: 9/200 Iteration: 1260 | Train loss: -33.03528\n",
      "Epoch: 9/200 Iteration: 1280 | Train loss: -66.65394\n",
      "Epoch: 9/200 Iteration: 1300 | Train loss: -41.94285\n",
      "Epoch: 9/200 Iteration: 1320 | Train loss: -45.66903\n",
      "Epoch: 9/200 Iteration: 1340 | Train loss: -63.60033\n",
      "Epoch: 9/200 Iteration: 1360 | Train loss: -46.62011\n",
      "Epoch: 9/200 Iteration: 1380 | Train loss: -67.86150\n",
      "Epoch: 10/200 Iteration: 1400 | Train loss: -66.13689\n",
      "Epoch: 10/200 Iteration: 1420 | Train loss: -51.99570\n",
      "Epoch: 10/200 Iteration: 1440 | Train loss: -45.80287\n",
      "Epoch: 10/200 Iteration: 1460 | Train loss: -40.43977\n",
      "Epoch: 10/200 Iteration: 1480 | Train loss: -62.41072\n",
      "Epoch: 10/200 Iteration: 1500 | Train loss: -37.19989\n",
      "Epoch: 10/200 Iteration: 1520 | Train loss: -64.40532\n",
      "Epoch: 10/200 Iteration: 1540 | Train loss: -61.96671\n",
      "Epoch: 11/200 Iteration: 1560 | Train loss: -62.98417\n",
      "Epoch: 11/200 Iteration: 1580 | Train loss: -80.24174\n",
      "Epoch: 11/200 Iteration: 1600 | Train loss: -72.29642\n",
      "Epoch: 11/200 Iteration: 1620 | Train loss: -57.70198\n",
      "Epoch: 11/200 Iteration: 1640 | Train loss: -39.41811\n",
      "Epoch: 11/200 Iteration: 1660 | Train loss: -59.94239\n",
      "Epoch: 11/200 Iteration: 1680 | Train loss: -67.79053\n",
      "Epoch: 11/200 Iteration: 1700 | Train loss: -80.64348\n",
      "Epoch: 12/200 Iteration: 1720 | Train loss: -59.99829\n",
      "Epoch: 12/200 Iteration: 1740 | Train loss: -73.54673\n",
      "Epoch: 12/200 Iteration: 1760 | Train loss: -73.02210\n",
      "Epoch: 12/200 Iteration: 1780 | Train loss: -59.05031\n",
      "Epoch: 12/200 Iteration: 1800 | Train loss: -61.41192\n",
      "Epoch: 12/200 Iteration: 1820 | Train loss: -75.13364\n",
      "Epoch: 12/200 Iteration: 1840 | Train loss: -69.32753\n",
      "Epoch: 12/200 Iteration: 1860 | Train loss: -72.79289\n",
      "Epoch: 13/200 Iteration: 1880 | Train loss: -53.76552\n",
      "Epoch: 13/200 Iteration: 1900 | Train loss: -95.62560\n",
      "Epoch: 13/200 Iteration: 1920 | Train loss: -59.64832\n",
      "Epoch: 13/200 Iteration: 1940 | Train loss: -69.81614\n",
      "Epoch: 13/200 Iteration: 1960 | Train loss: -91.72471\n",
      "Epoch: 13/200 Iteration: 1980 | Train loss: -70.68803\n",
      "Epoch: 13/200 Iteration: 2000 | Train loss: -95.52287\n",
      "Epoch: 14/200 Iteration: 2020 | Train loss: -98.85952\n",
      "Epoch: 14/200 Iteration: 2040 | Train loss: -74.47408\n",
      "Epoch: 14/200 Iteration: 2060 | Train loss: -71.34144\n",
      "Epoch: 14/200 Iteration: 2080 | Train loss: -65.98458\n",
      "Epoch: 14/200 Iteration: 2100 | Train loss: -89.70381\n",
      "Epoch: 14/200 Iteration: 2120 | Train loss: -49.76654\n",
      "Epoch: 14/200 Iteration: 2140 | Train loss: -87.04506\n",
      "Epoch: 14/200 Iteration: 2160 | Train loss: -86.43203\n",
      "Epoch: 15/200 Iteration: 2180 | Train loss: -91.50562\n",
      "Epoch: 15/200 Iteration: 2200 | Train loss: -111.67209\n",
      "Epoch: 15/200 Iteration: 2220 | Train loss: -107.96567\n",
      "Epoch: 15/200 Iteration: 2240 | Train loss: -79.20425\n",
      "Epoch: 15/200 Iteration: 2260 | Train loss: -57.26927\n",
      "Epoch: 15/200 Iteration: 2280 | Train loss: -77.02693\n",
      "Epoch: 15/200 Iteration: 2300 | Train loss: -93.96498\n",
      "Epoch: 15/200 Iteration: 2320 | Train loss: -114.67422\n",
      "Epoch: 16/200 Iteration: 2340 | Train loss: -86.13425\n",
      "Epoch: 16/200 Iteration: 2360 | Train loss: -99.63515\n",
      "Epoch: 16/200 Iteration: 2380 | Train loss: -100.71067\n",
      "Epoch: 16/200 Iteration: 2400 | Train loss: -83.65947\n",
      "Epoch: 16/200 Iteration: 2420 | Train loss: -85.70776\n",
      "Epoch: 16/200 Iteration: 2440 | Train loss: -101.33646\n",
      "Epoch: 16/200 Iteration: 2460 | Train loss: -97.96531\n",
      "Epoch: 16/200 Iteration: 2480 | Train loss: -102.43098\n",
      "Epoch: 17/200 Iteration: 2500 | Train loss: -67.08373\n",
      "Epoch: 17/200 Iteration: 2520 | Train loss: -132.73701\n",
      "Epoch: 17/200 Iteration: 2540 | Train loss: -84.37288\n",
      "Epoch: 17/200 Iteration: 2560 | Train loss: -93.02248\n",
      "Epoch: 17/200 Iteration: 2580 | Train loss: -122.73325\n",
      "Epoch: 17/200 Iteration: 2600 | Train loss: -88.05891\n",
      "Epoch: 17/200 Iteration: 2620 | Train loss: -130.37408\n",
      "Epoch: 18/200 Iteration: 2640 | Train loss: -130.91301\n",
      "Epoch: 18/200 Iteration: 2660 | Train loss: -96.24242\n",
      "Epoch: 18/200 Iteration: 2680 | Train loss: -95.63334\n",
      "Epoch: 18/200 Iteration: 2700 | Train loss: -82.25282\n",
      "Epoch: 18/200 Iteration: 2720 | Train loss: -127.16807\n",
      "Epoch: 18/200 Iteration: 2740 | Train loss: -72.41735\n",
      "Epoch: 18/200 Iteration: 2760 | Train loss: -111.73009\n",
      "Epoch: 18/200 Iteration: 2780 | Train loss: -115.67146\n",
      "Epoch: 19/200 Iteration: 2800 | Train loss: -119.46258\n",
      "Epoch: 19/200 Iteration: 2820 | Train loss: -145.61963\n",
      "Epoch: 19/200 Iteration: 2840 | Train loss: -138.13039\n",
      "Epoch: 19/200 Iteration: 2860 | Train loss: -99.60549\n",
      "Epoch: 19/200 Iteration: 2880 | Train loss: -70.86517\n",
      "Epoch: 19/200 Iteration: 2900 | Train loss: -103.02871\n",
      "Epoch: 19/200 Iteration: 2920 | Train loss: -127.96304\n",
      "Epoch: 19/200 Iteration: 2940 | Train loss: -141.81197\n",
      "Epoch: 20/200 Iteration: 2960 | Train loss: -111.56502\n",
      "Epoch: 20/200 Iteration: 2980 | Train loss: -125.17209\n",
      "Epoch: 20/200 Iteration: 3000 | Train loss: -127.21705\n",
      "Epoch: 20/200 Iteration: 3020 | Train loss: -111.01830\n",
      "Epoch: 20/200 Iteration: 3040 | Train loss: -105.29590\n",
      "Epoch: 20/200 Iteration: 3060 | Train loss: -124.18821\n",
      "Epoch: 20/200 Iteration: 3080 | Train loss: -129.19508\n",
      "Epoch: 20/200 Iteration: 3100 | Train loss: -131.23538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/200 Iteration: 3120 | Train loss: -84.72800\n",
      "Epoch: 21/200 Iteration: 3140 | Train loss: -158.84778\n",
      "Epoch: 21/200 Iteration: 3160 | Train loss: -102.89279\n",
      "Epoch: 21/200 Iteration: 3180 | Train loss: -112.02808\n",
      "Epoch: 21/200 Iteration: 3200 | Train loss: -151.55048\n",
      "Epoch: 21/200 Iteration: 3220 | Train loss: -116.63463\n",
      "Epoch: 21/200 Iteration: 3240 | Train loss: -149.84395\n",
      "Epoch: 22/200 Iteration: 3260 | Train loss: -156.54411\n",
      "Epoch: 22/200 Iteration: 3280 | Train loss: -116.06241\n",
      "Epoch: 22/200 Iteration: 3300 | Train loss: -118.04439\n",
      "Epoch: 22/200 Iteration: 3320 | Train loss: -98.18291\n",
      "Epoch: 22/200 Iteration: 3340 | Train loss: -149.24576\n",
      "Epoch: 22/200 Iteration: 3360 | Train loss: -86.41217\n",
      "Epoch: 22/200 Iteration: 3380 | Train loss: -138.67598\n",
      "Epoch: 22/200 Iteration: 3400 | Train loss: -138.65640\n",
      "Epoch: 23/200 Iteration: 3420 | Train loss: -140.86830\n",
      "Epoch: 23/200 Iteration: 3440 | Train loss: -160.39996\n",
      "Epoch: 23/200 Iteration: 3460 | Train loss: -154.43222\n",
      "Epoch: 23/200 Iteration: 3480 | Train loss: -121.93678\n",
      "Epoch: 23/200 Iteration: 3500 | Train loss: -86.17847\n",
      "Epoch: 23/200 Iteration: 3520 | Train loss: -123.49718\n",
      "Epoch: 23/200 Iteration: 3540 | Train loss: -150.41721\n",
      "Epoch: 23/200 Iteration: 3560 | Train loss: -168.02130\n",
      "Epoch: 24/200 Iteration: 3580 | Train loss: -141.07951\n",
      "Epoch: 24/200 Iteration: 3600 | Train loss: -150.39352\n",
      "Epoch: 24/200 Iteration: 3620 | Train loss: -156.54314\n",
      "Epoch: 24/200 Iteration: 3640 | Train loss: -43.40378\n",
      "Epoch: 24/200 Iteration: 3660 | Train loss: -101.38155\n",
      "Epoch: 24/200 Iteration: 3680 | Train loss: -136.56181\n",
      "Epoch: 24/200 Iteration: 3700 | Train loss: -144.35507\n",
      "Epoch: 24/200 Iteration: 3720 | Train loss: -138.43149\n",
      "Epoch: 25/200 Iteration: 3740 | Train loss: -100.64546\n",
      "Epoch: 25/200 Iteration: 3760 | Train loss: -185.77074\n",
      "Epoch: 25/200 Iteration: 3780 | Train loss: -111.40818\n",
      "Epoch: 25/200 Iteration: 3800 | Train loss: -132.17973\n",
      "Epoch: 25/200 Iteration: 3820 | Train loss: -173.32770\n",
      "Epoch: 25/200 Iteration: 3840 | Train loss: -140.73070\n",
      "Epoch: 25/200 Iteration: 3860 | Train loss: -178.94609\n",
      "Epoch: 26/200 Iteration: 3880 | Train loss: -183.76244\n",
      "Epoch: 26/200 Iteration: 3900 | Train loss: -135.36162\n",
      "Epoch: 26/200 Iteration: 3920 | Train loss: -133.64049\n",
      "Epoch: 26/200 Iteration: 3940 | Train loss: -115.86205\n",
      "Epoch: 26/200 Iteration: 3960 | Train loss: -173.87753\n",
      "Epoch: 26/200 Iteration: 3980 | Train loss: -102.38641\n",
      "Epoch: 26/200 Iteration: 4000 | Train loss: -161.61066\n",
      "Epoch: 26/200 Iteration: 4020 | Train loss: -164.20660\n",
      "Epoch: 27/200 Iteration: 4040 | Train loss: -165.51227\n",
      "Epoch: 27/200 Iteration: 4060 | Train loss: -204.50159\n",
      "Epoch: 27/200 Iteration: 4080 | Train loss: -190.48865\n",
      "Epoch: 27/200 Iteration: 4100 | Train loss: -144.39651\n",
      "Epoch: 27/200 Iteration: 4120 | Train loss: -93.47321\n",
      "Epoch: 27/200 Iteration: 4140 | Train loss: -145.78162\n",
      "Epoch: 27/200 Iteration: 4160 | Train loss: -175.46353\n",
      "Epoch: 27/200 Iteration: 4180 | Train loss: -197.15945\n",
      "Epoch: 28/200 Iteration: 4200 | Train loss: -162.06662\n",
      "Epoch: 28/200 Iteration: 4220 | Train loss: -171.90680\n",
      "Epoch: 28/200 Iteration: 4240 | Train loss: -176.81592\n",
      "Epoch: 28/200 Iteration: 4260 | Train loss: -151.61105\n",
      "Epoch: 28/200 Iteration: 4280 | Train loss: -143.18134\n",
      "Epoch: 28/200 Iteration: 4300 | Train loss: -165.45097\n",
      "Epoch: 28/200 Iteration: 4320 | Train loss: -170.95966\n",
      "Epoch: 28/200 Iteration: 4340 | Train loss: -178.80997\n",
      "Epoch: 29/200 Iteration: 4360 | Train loss: -123.46523\n",
      "Epoch: 29/200 Iteration: 4380 | Train loss: -219.85246\n",
      "Epoch: 29/200 Iteration: 4400 | Train loss: -144.36546\n",
      "Epoch: 29/200 Iteration: 4420 | Train loss: -152.48589\n",
      "Epoch: 29/200 Iteration: 4440 | Train loss: -206.78525\n",
      "Epoch: 29/200 Iteration: 4460 | Train loss: -165.29877\n",
      "Epoch: 29/200 Iteration: 4480 | Train loss: -207.03831\n",
      "Epoch: 30/200 Iteration: 4500 | Train loss: -219.50591\n",
      "Epoch: 30/200 Iteration: 4520 | Train loss: -151.83694\n",
      "Epoch: 30/200 Iteration: 4540 | Train loss: -157.11884\n",
      "Epoch: 30/200 Iteration: 4560 | Train loss: -138.30742\n",
      "Epoch: 30/200 Iteration: 4580 | Train loss: -202.28511\n",
      "Epoch: 30/200 Iteration: 4600 | Train loss: -114.43037\n",
      "Epoch: 30/200 Iteration: 4620 | Train loss: -184.16597\n",
      "Epoch: 30/200 Iteration: 4640 | Train loss: -187.63567\n",
      "Epoch: 31/200 Iteration: 4660 | Train loss: -196.77977\n",
      "Epoch: 31/200 Iteration: 4680 | Train loss: -243.26346\n",
      "Epoch: 31/200 Iteration: 4700 | Train loss: -224.70845\n",
      "Epoch: 31/200 Iteration: 4720 | Train loss: -160.66693\n",
      "Epoch: 31/200 Iteration: 4740 | Train loss: -117.64362\n",
      "Epoch: 31/200 Iteration: 4760 | Train loss: -163.72430\n",
      "Epoch: 31/200 Iteration: 4780 | Train loss: -200.88698\n",
      "Epoch: 31/200 Iteration: 4800 | Train loss: -223.41988\n",
      "Epoch: 32/200 Iteration: 4820 | Train loss: -182.60562\n",
      "Epoch: 32/200 Iteration: 4840 | Train loss: -194.27199\n",
      "Epoch: 32/200 Iteration: 4860 | Train loss: -198.19965\n",
      "Epoch: 32/200 Iteration: 4880 | Train loss: -168.65912\n",
      "Epoch: 32/200 Iteration: 4900 | Train loss: -157.25732\n",
      "Epoch: 32/200 Iteration: 4920 | Train loss: -192.11407\n",
      "Epoch: 32/200 Iteration: 4940 | Train loss: -196.35667\n",
      "Epoch: 32/200 Iteration: 4960 | Train loss: -206.97028\n",
      "Epoch: 33/200 Iteration: 4980 | Train loss: -142.21590\n",
      "Epoch: 33/200 Iteration: 5000 | Train loss: -253.69701\n",
      "Epoch: 33/200 Iteration: 5020 | Train loss: -163.02858\n",
      "Epoch: 33/200 Iteration: 5040 | Train loss: -173.36746\n",
      "Epoch: 33/200 Iteration: 5060 | Train loss: -234.01424\n",
      "Epoch: 33/200 Iteration: 5080 | Train loss: -190.24342\n",
      "Epoch: 33/200 Iteration: 5100 | Train loss: -234.88605\n",
      "Epoch: 34/200 Iteration: 5120 | Train loss: -248.38414\n",
      "Epoch: 34/200 Iteration: 5140 | Train loss: -167.38895\n",
      "Epoch: 34/200 Iteration: 5160 | Train loss: -179.15848\n",
      "Epoch: 34/200 Iteration: 5180 | Train loss: -147.38589\n",
      "Epoch: 34/200 Iteration: 5200 | Train loss: -239.47461\n",
      "Epoch: 34/200 Iteration: 5220 | Train loss: -129.74263\n",
      "Epoch: 34/200 Iteration: 5240 | Train loss: -207.86386\n",
      "Epoch: 34/200 Iteration: 5260 | Train loss: -203.42035\n",
      "Epoch: 35/200 Iteration: 5280 | Train loss: -213.86145\n",
      "Epoch: 35/200 Iteration: 5300 | Train loss: -264.87210\n",
      "Epoch: 35/200 Iteration: 5320 | Train loss: -244.19078\n",
      "Epoch: 35/200 Iteration: 5340 | Train loss: -185.90878\n",
      "Epoch: 35/200 Iteration: 5360 | Train loss: -128.30028\n",
      "Epoch: 35/200 Iteration: 5380 | Train loss: -185.16058\n",
      "Epoch: 35/200 Iteration: 5400 | Train loss: -223.24393\n",
      "Epoch: 35/200 Iteration: 5420 | Train loss: -257.20303\n",
      "Epoch: 36/200 Iteration: 5440 | Train loss: -211.21359\n",
      "Epoch: 36/200 Iteration: 5460 | Train loss: -211.71489\n",
      "Epoch: 36/200 Iteration: 5480 | Train loss: -230.42905\n",
      "Epoch: 36/200 Iteration: 5500 | Train loss: -184.90160\n",
      "Epoch: 36/200 Iteration: 5520 | Train loss: -183.79659\n",
      "Epoch: 36/200 Iteration: 5540 | Train loss: -215.48940\n",
      "Epoch: 36/200 Iteration: 5560 | Train loss: -219.46909\n",
      "Epoch: 36/200 Iteration: 5580 | Train loss: -226.43254\n",
      "Epoch: 37/200 Iteration: 5600 | Train loss: -146.95110\n",
      "Epoch: 37/200 Iteration: 5620 | Train loss: -280.23618\n",
      "Epoch: 37/200 Iteration: 5640 | Train loss: -189.70969\n",
      "Epoch: 37/200 Iteration: 5660 | Train loss: -197.13121\n",
      "Epoch: 37/200 Iteration: 5680 | Train loss: -257.89053\n",
      "Epoch: 37/200 Iteration: 5700 | Train loss: -205.23027\n",
      "Epoch: 37/200 Iteration: 5720 | Train loss: -264.22641\n",
      "Epoch: 38/200 Iteration: 5740 | Train loss: -276.30548\n",
      "Epoch: 38/200 Iteration: 5760 | Train loss: -201.45660\n",
      "Epoch: 38/200 Iteration: 5780 | Train loss: -192.62762\n",
      "Epoch: 38/200 Iteration: 5800 | Train loss: -170.52519\n",
      "Epoch: 38/200 Iteration: 5820 | Train loss: -262.51620\n",
      "Epoch: 38/200 Iteration: 5840 | Train loss: -141.20284\n",
      "Epoch: 38/200 Iteration: 5860 | Train loss: -230.19525\n",
      "Epoch: 38/200 Iteration: 5880 | Train loss: -225.93660\n",
      "Epoch: 39/200 Iteration: 5900 | Train loss: -238.55949\n",
      "Epoch: 39/200 Iteration: 5920 | Train loss: -281.40393\n",
      "Epoch: 39/200 Iteration: 5940 | Train loss: -277.86172\n",
      "Epoch: 39/200 Iteration: 5960 | Train loss: -204.64215\n",
      "Epoch: 39/200 Iteration: 5980 | Train loss: -138.95549\n",
      "Epoch: 39/200 Iteration: 6000 | Train loss: -195.82640\n",
      "Epoch: 39/200 Iteration: 6020 | Train loss: -253.66560\n",
      "Epoch: 39/200 Iteration: 6040 | Train loss: -278.56204\n",
      "Epoch: 40/200 Iteration: 6060 | Train loss: -233.00308\n",
      "Epoch: 40/200 Iteration: 6080 | Train loss: -239.27655\n",
      "Epoch: 40/200 Iteration: 6100 | Train loss: -257.23956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/200 Iteration: 6120 | Train loss: -210.45137\n",
      "Epoch: 40/200 Iteration: 6140 | Train loss: -203.13275\n",
      "Epoch: 40/200 Iteration: 6160 | Train loss: -238.79562\n",
      "Epoch: 40/200 Iteration: 6180 | Train loss: -241.44379\n",
      "Epoch: 40/200 Iteration: 6200 | Train loss: -259.45413\n",
      "Epoch: 41/200 Iteration: 6220 | Train loss: -168.92656\n",
      "Epoch: 41/200 Iteration: 6240 | Train loss: -312.66031\n",
      "Epoch: 41/200 Iteration: 6260 | Train loss: -204.52660\n",
      "Epoch: 41/200 Iteration: 6280 | Train loss: -207.61021\n",
      "Epoch: 41/200 Iteration: 6300 | Train loss: -286.19144\n",
      "Epoch: 41/200 Iteration: 6320 | Train loss: -234.68797\n",
      "Epoch: 41/200 Iteration: 6340 | Train loss: -293.24850\n",
      "Epoch: 42/200 Iteration: 6360 | Train loss: -297.83884\n",
      "Epoch: 42/200 Iteration: 6380 | Train loss: -214.51717\n",
      "Epoch: 42/200 Iteration: 6400 | Train loss: -221.60999\n",
      "Epoch: 42/200 Iteration: 6420 | Train loss: -178.72217\n",
      "Epoch: 42/200 Iteration: 6440 | Train loss: -282.05109\n",
      "Epoch: 42/200 Iteration: 6460 | Train loss: -164.21152\n",
      "Epoch: 42/200 Iteration: 6480 | Train loss: -251.62805\n",
      "Epoch: 42/200 Iteration: 6500 | Train loss: -259.04184\n",
      "Epoch: 43/200 Iteration: 6520 | Train loss: -263.03003\n",
      "Epoch: 43/200 Iteration: 6540 | Train loss: -319.33325\n",
      "Epoch: 43/200 Iteration: 6560 | Train loss: -305.79691\n",
      "Epoch: 43/200 Iteration: 6580 | Train loss: -221.52036\n",
      "Epoch: 43/200 Iteration: 6600 | Train loss: -158.27119\n",
      "Epoch: 43/200 Iteration: 6620 | Train loss: -226.24437\n",
      "Epoch: 43/200 Iteration: 6640 | Train loss: -281.03320\n",
      "Epoch: 43/200 Iteration: 6660 | Train loss: -304.72543\n",
      "Epoch: 44/200 Iteration: 6680 | Train loss: -257.64944\n",
      "Epoch: 44/200 Iteration: 6700 | Train loss: -264.23718\n",
      "Epoch: 44/200 Iteration: 6720 | Train loss: -286.47977\n",
      "Epoch: 44/200 Iteration: 6740 | Train loss: -232.89954\n",
      "Epoch: 44/200 Iteration: 6760 | Train loss: -227.03397\n",
      "Epoch: 44/200 Iteration: 6780 | Train loss: -266.54706\n",
      "Epoch: 44/200 Iteration: 6800 | Train loss: -264.98257\n",
      "Epoch: 44/200 Iteration: 6820 | Train loss: -274.76099\n",
      "Epoch: 45/200 Iteration: 6840 | Train loss: -185.83797\n",
      "Epoch: 45/200 Iteration: 6860 | Train loss: -347.94080\n",
      "Epoch: 45/200 Iteration: 6880 | Train loss: -219.67796\n",
      "Epoch: 45/200 Iteration: 6900 | Train loss: -237.88712\n",
      "Epoch: 45/200 Iteration: 6920 | Train loss: -315.67191\n",
      "Epoch: 45/200 Iteration: 6940 | Train loss: -251.69366\n",
      "Epoch: 45/200 Iteration: 6960 | Train loss: -320.15942\n",
      "Epoch: 46/200 Iteration: 6980 | Train loss: -343.60858\n",
      "Epoch: 46/200 Iteration: 7000 | Train loss: -246.44722\n",
      "Epoch: 46/200 Iteration: 7020 | Train loss: -234.76416\n",
      "Epoch: 46/200 Iteration: 7040 | Train loss: -206.28215\n",
      "Epoch: 46/200 Iteration: 7060 | Train loss: -309.98080\n",
      "Epoch: 46/200 Iteration: 7080 | Train loss: -174.23355\n",
      "Epoch: 46/200 Iteration: 7100 | Train loss: -275.57684\n",
      "Epoch: 46/200 Iteration: 7120 | Train loss: -270.76431\n",
      "Epoch: 47/200 Iteration: 7140 | Train loss: -281.72772\n",
      "Epoch: 47/200 Iteration: 7160 | Train loss: -352.49374\n",
      "Epoch: 47/200 Iteration: 7180 | Train loss: -330.03210\n",
      "Epoch: 47/200 Iteration: 7200 | Train loss: -239.00977\n",
      "Epoch: 47/200 Iteration: 7220 | Train loss: -171.32149\n",
      "Epoch: 47/200 Iteration: 7240 | Train loss: -234.98152\n",
      "Epoch: 47/200 Iteration: 7260 | Train loss: -304.92700\n",
      "Epoch: 47/200 Iteration: 7280 | Train loss: -343.95468\n",
      "Epoch: 48/200 Iteration: 7300 | Train loss: -272.34210\n",
      "Epoch: 48/200 Iteration: 7320 | Train loss: -281.01321\n",
      "Epoch: 48/200 Iteration: 7340 | Train loss: -307.70880\n",
      "Epoch: 48/200 Iteration: 7360 | Train loss: -237.98586\n",
      "Epoch: 48/200 Iteration: 7380 | Train loss: -240.88551\n",
      "Epoch: 48/200 Iteration: 7400 | Train loss: -294.43210\n",
      "Epoch: 48/200 Iteration: 7420 | Train loss: -288.77673\n",
      "Epoch: 48/200 Iteration: 7440 | Train loss: -296.83954\n",
      "Epoch: 49/200 Iteration: 7460 | Train loss: -200.91296\n",
      "Epoch: 49/200 Iteration: 7480 | Train loss: -378.82217\n",
      "Epoch: 49/200 Iteration: 7500 | Train loss: -242.61075\n",
      "Epoch: 49/200 Iteration: 7520 | Train loss: -256.17471\n",
      "Epoch: 49/200 Iteration: 7540 | Train loss: -344.60886\n",
      "Epoch: 49/200 Iteration: 7560 | Train loss: -277.57883\n",
      "Epoch: 49/200 Iteration: 7580 | Train loss: -339.30173\n",
      "Epoch: 50/200 Iteration: 7600 | Train loss: -367.78421\n",
      "Epoch: 50/200 Iteration: 7620 | Train loss: -252.55336\n",
      "Epoch: 50/200 Iteration: 7640 | Train loss: -253.60172\n",
      "Epoch: 50/200 Iteration: 7660 | Train loss: -232.00739\n",
      "Epoch: 50/200 Iteration: 7680 | Train loss: -333.76562\n",
      "Epoch: 50/200 Iteration: 7700 | Train loss: -190.26460\n",
      "Epoch: 50/200 Iteration: 7720 | Train loss: -292.95776\n",
      "Epoch: 50/200 Iteration: 7740 | Train loss: -297.67380\n",
      "Epoch: 51/200 Iteration: 7760 | Train loss: -317.76877\n",
      "Epoch: 51/200 Iteration: 7780 | Train loss: -379.45200\n",
      "Epoch: 51/200 Iteration: 7800 | Train loss: -363.04599\n",
      "Epoch: 51/200 Iteration: 7820 | Train loss: -268.65204\n",
      "Epoch: 51/200 Iteration: 7840 | Train loss: -184.57562\n",
      "Epoch: 51/200 Iteration: 7860 | Train loss: -265.92450\n",
      "Epoch: 51/200 Iteration: 7880 | Train loss: -330.41840\n",
      "Epoch: 51/200 Iteration: 7900 | Train loss: -369.90009\n",
      "Epoch: 52/200 Iteration: 7920 | Train loss: -297.16144\n",
      "Epoch: 52/200 Iteration: 7940 | Train loss: -306.89435\n",
      "Epoch: 52/200 Iteration: 7960 | Train loss: -314.28735\n",
      "Epoch: 52/200 Iteration: 7980 | Train loss: -264.70050\n",
      "Epoch: 52/200 Iteration: 8000 | Train loss: -247.49942\n",
      "Epoch: 52/200 Iteration: 8020 | Train loss: -312.50977\n",
      "Epoch: 52/200 Iteration: 8040 | Train loss: -316.59314\n",
      "Epoch: 52/200 Iteration: 8060 | Train loss: -319.37421\n",
      "Epoch: 53/200 Iteration: 8080 | Train loss: -219.29849\n",
      "Epoch: 53/200 Iteration: 8100 | Train loss: -391.94101\n",
      "Epoch: 53/200 Iteration: 8120 | Train loss: -264.39999\n",
      "Epoch: 53/200 Iteration: 8140 | Train loss: -276.28891\n",
      "Epoch: 53/200 Iteration: 8160 | Train loss: -365.17819\n",
      "Epoch: 53/200 Iteration: 8180 | Train loss: -297.97989\n",
      "Epoch: 53/200 Iteration: 8200 | Train loss: -355.39514\n",
      "Epoch: 54/200 Iteration: 8220 | Train loss: -395.11234\n",
      "Epoch: 54/200 Iteration: 8240 | Train loss: -278.08261\n",
      "Epoch: 54/200 Iteration: 8260 | Train loss: -277.26181\n",
      "Epoch: 54/200 Iteration: 8280 | Train loss: -234.26151\n",
      "Epoch: 54/200 Iteration: 8300 | Train loss: -364.17032\n",
      "Epoch: 54/200 Iteration: 8320 | Train loss: -200.87529\n",
      "Epoch: 54/200 Iteration: 8340 | Train loss: -318.61749\n",
      "Epoch: 54/200 Iteration: 8360 | Train loss: -328.29846\n",
      "Epoch: 55/200 Iteration: 8380 | Train loss: -331.80289\n",
      "Epoch: 55/200 Iteration: 8400 | Train loss: -394.70044\n",
      "Epoch: 55/200 Iteration: 8420 | Train loss: -386.77008\n",
      "Epoch: 55/200 Iteration: 8440 | Train loss: -286.33890\n",
      "Epoch: 55/200 Iteration: 8460 | Train loss: -198.68730\n",
      "Epoch: 55/200 Iteration: 8480 | Train loss: -279.62518\n",
      "Epoch: 55/200 Iteration: 8500 | Train loss: -360.69229\n",
      "Epoch: 55/200 Iteration: 8520 | Train loss: -396.15216\n",
      "Epoch: 56/200 Iteration: 8540 | Train loss: -316.76251\n",
      "Epoch: 56/200 Iteration: 8560 | Train loss: -333.03094\n",
      "Epoch: 56/200 Iteration: 8580 | Train loss: -354.86835\n",
      "Epoch: 56/200 Iteration: 8600 | Train loss: -278.53262\n",
      "Epoch: 56/200 Iteration: 8620 | Train loss: -282.20889\n",
      "Epoch: 56/200 Iteration: 8640 | Train loss: -341.33673\n",
      "Epoch: 56/200 Iteration: 8660 | Train loss: -333.40308\n",
      "Epoch: 56/200 Iteration: 8680 | Train loss: -343.32285\n",
      "Epoch: 57/200 Iteration: 8700 | Train loss: -236.33949\n",
      "Epoch: 57/200 Iteration: 8720 | Train loss: -415.39914\n",
      "Epoch: 57/200 Iteration: 8740 | Train loss: -288.12366\n",
      "Epoch: 57/200 Iteration: 8760 | Train loss: -295.50443\n",
      "Epoch: 57/200 Iteration: 8780 | Train loss: -404.03821\n",
      "Epoch: 57/200 Iteration: 8800 | Train loss: -298.61926\n",
      "Epoch: 57/200 Iteration: 8820 | Train loss: -404.00040\n",
      "Epoch: 58/200 Iteration: 8840 | Train loss: -411.46524\n",
      "Epoch: 58/200 Iteration: 8860 | Train loss: -305.86230\n",
      "Epoch: 58/200 Iteration: 8880 | Train loss: -303.13141\n",
      "Epoch: 58/200 Iteration: 8900 | Train loss: -256.35727\n",
      "Epoch: 58/200 Iteration: 8920 | Train loss: -381.76132\n",
      "Epoch: 58/200 Iteration: 8940 | Train loss: -227.05331\n",
      "Epoch: 58/200 Iteration: 8960 | Train loss: -350.97665\n",
      "Epoch: 58/200 Iteration: 8980 | Train loss: -349.52637\n",
      "Epoch: 59/200 Iteration: 9000 | Train loss: -349.54449\n",
      "Epoch: 59/200 Iteration: 9020 | Train loss: -430.38361\n",
      "Epoch: 59/200 Iteration: 9040 | Train loss: -420.37555\n",
      "Epoch: 59/200 Iteration: 9060 | Train loss: -300.95255\n",
      "Epoch: 59/200 Iteration: 9080 | Train loss: -212.15271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/200 Iteration: 9100 | Train loss: -305.66269\n",
      "Epoch: 59/200 Iteration: 9120 | Train loss: -378.29504\n",
      "Epoch: 59/200 Iteration: 9140 | Train loss: -421.01102\n",
      "Epoch: 60/200 Iteration: 9160 | Train loss: -328.42242\n",
      "Epoch: 60/200 Iteration: 9180 | Train loss: -352.84875\n",
      "Epoch: 60/200 Iteration: 9200 | Train loss: -378.16086\n",
      "Epoch: 60/200 Iteration: 9220 | Train loss: -309.84204\n",
      "Epoch: 60/200 Iteration: 9240 | Train loss: -292.35358\n",
      "Epoch: 60/200 Iteration: 9260 | Train loss: -362.27985\n",
      "Epoch: 60/200 Iteration: 9280 | Train loss: -359.94772\n",
      "Epoch: 60/200 Iteration: 9300 | Train loss: -370.31821\n",
      "Epoch: 61/200 Iteration: 9320 | Train loss: -257.53232\n",
      "Epoch: 61/200 Iteration: 9340 | Train loss: -461.70844\n",
      "Epoch: 61/200 Iteration: 9360 | Train loss: -317.67194\n",
      "Epoch: 61/200 Iteration: 9380 | Train loss: -312.26242\n",
      "Epoch: 61/200 Iteration: 9400 | Train loss: -413.22437\n",
      "Epoch: 61/200 Iteration: 9420 | Train loss: -342.39243\n",
      "Epoch: 61/200 Iteration: 9440 | Train loss: -418.99228\n",
      "Epoch: 62/200 Iteration: 9460 | Train loss: -446.52774\n",
      "Epoch: 62/200 Iteration: 9480 | Train loss: -318.98392\n",
      "Epoch: 62/200 Iteration: 9500 | Train loss: -320.17578\n",
      "Epoch: 62/200 Iteration: 9520 | Train loss: -269.36270\n",
      "Epoch: 62/200 Iteration: 9540 | Train loss: -408.01697\n",
      "Epoch: 62/200 Iteration: 9560 | Train loss: -242.91042\n",
      "Epoch: 62/200 Iteration: 9580 | Train loss: -369.94202\n",
      "Epoch: 62/200 Iteration: 9600 | Train loss: -370.04404\n",
      "Epoch: 63/200 Iteration: 9620 | Train loss: -384.17914\n",
      "Epoch: 63/200 Iteration: 9640 | Train loss: -469.59827\n",
      "Epoch: 63/200 Iteration: 9660 | Train loss: -437.25980\n",
      "Epoch: 63/200 Iteration: 9680 | Train loss: -315.45535\n",
      "Epoch: 63/200 Iteration: 9700 | Train loss: -231.11794\n",
      "Epoch: 63/200 Iteration: 9720 | Train loss: -328.74023\n",
      "Epoch: 63/200 Iteration: 9740 | Train loss: -400.32767\n",
      "Epoch: 63/200 Iteration: 9760 | Train loss: -442.62430\n",
      "Epoch: 64/200 Iteration: 9780 | Train loss: -355.49774\n",
      "Epoch: 64/200 Iteration: 9800 | Train loss: -379.35715\n",
      "Epoch: 64/200 Iteration: 9820 | Train loss: -396.09189\n",
      "Epoch: 64/200 Iteration: 9840 | Train loss: -335.38110\n",
      "Epoch: 64/200 Iteration: 9860 | Train loss: -321.06406\n",
      "Epoch: 64/200 Iteration: 9880 | Train loss: -368.62589\n",
      "Epoch: 64/200 Iteration: 9900 | Train loss: -381.62497\n",
      "Epoch: 64/200 Iteration: 9920 | Train loss: -391.25381\n",
      "Epoch: 65/200 Iteration: 9940 | Train loss: -266.73608\n",
      "Epoch: 65/200 Iteration: 9960 | Train loss: -480.07339\n",
      "Epoch: 65/200 Iteration: 9980 | Train loss: -328.28674\n",
      "Epoch: 65/200 Iteration: 10000 | Train loss: -351.10336\n",
      "Epoch: 65/200 Iteration: 10020 | Train loss: -439.44156\n",
      "Epoch: 65/200 Iteration: 10040 | Train loss: -364.74011\n",
      "Epoch: 65/200 Iteration: 10060 | Train loss: -450.76743\n",
      "Epoch: 66/200 Iteration: 10080 | Train loss: -468.36453\n",
      "Epoch: 66/200 Iteration: 10100 | Train loss: -345.99280\n",
      "Epoch: 66/200 Iteration: 10120 | Train loss: -340.51837\n",
      "Epoch: 66/200 Iteration: 10140 | Train loss: -289.36023\n",
      "Epoch: 66/200 Iteration: 10160 | Train loss: -445.32727\n",
      "Epoch: 66/200 Iteration: 10180 | Train loss: -265.51257\n",
      "Epoch: 66/200 Iteration: 10200 | Train loss: -390.58368\n",
      "Epoch: 66/200 Iteration: 10220 | Train loss: -386.47882\n",
      "Epoch: 67/200 Iteration: 10240 | Train loss: -398.91367\n",
      "Epoch: 67/200 Iteration: 10260 | Train loss: -502.38068\n",
      "Epoch: 67/200 Iteration: 10280 | Train loss: -468.72186\n",
      "Epoch: 67/200 Iteration: 10300 | Train loss: -338.82776\n",
      "Epoch: 67/200 Iteration: 10320 | Train loss: -240.11293\n",
      "Epoch: 67/200 Iteration: 10340 | Train loss: -332.88391\n",
      "Epoch: 67/200 Iteration: 10360 | Train loss: -416.10864\n",
      "Epoch: 67/200 Iteration: 10380 | Train loss: -466.44577\n",
      "Epoch: 68/200 Iteration: 10400 | Train loss: -382.32211\n",
      "Epoch: 68/200 Iteration: 10420 | Train loss: -404.13333\n",
      "Epoch: 68/200 Iteration: 10440 | Train loss: -433.40094\n",
      "Epoch: 68/200 Iteration: 10460 | Train loss: -344.47931\n",
      "Epoch: 68/200 Iteration: 10480 | Train loss: -338.87106\n",
      "Epoch: 68/200 Iteration: 10500 | Train loss: -419.94241\n",
      "Epoch: 68/200 Iteration: 10520 | Train loss: -404.86282\n",
      "Epoch: 68/200 Iteration: 10540 | Train loss: -423.80200\n",
      "Epoch: 69/200 Iteration: 10560 | Train loss: -289.41721\n",
      "Epoch: 69/200 Iteration: 10580 | Train loss: -517.83099\n",
      "Epoch: 69/200 Iteration: 10600 | Train loss: -336.01532\n",
      "Epoch: 69/200 Iteration: 10620 | Train loss: -354.23743\n",
      "Epoch: 69/200 Iteration: 10640 | Train loss: -469.39337\n",
      "Epoch: 69/200 Iteration: 10660 | Train loss: -391.13318\n",
      "Epoch: 69/200 Iteration: 10680 | Train loss: -481.13062\n",
      "Epoch: 70/200 Iteration: 10700 | Train loss: -516.29260\n",
      "Epoch: 70/200 Iteration: 10720 | Train loss: -373.33734\n",
      "Epoch: 70/200 Iteration: 10740 | Train loss: -360.06177\n",
      "Epoch: 70/200 Iteration: 10760 | Train loss: -302.14691\n",
      "Epoch: 70/200 Iteration: 10780 | Train loss: -469.85706\n",
      "Epoch: 70/200 Iteration: 10800 | Train loss: -280.45108\n",
      "Epoch: 70/200 Iteration: 10820 | Train loss: -410.05408\n",
      "Epoch: 70/200 Iteration: 10840 | Train loss: -415.17090\n",
      "Epoch: 71/200 Iteration: 10860 | Train loss: -442.31897\n",
      "Epoch: 71/200 Iteration: 10880 | Train loss: -521.57037\n",
      "Epoch: 71/200 Iteration: 10900 | Train loss: -502.26428\n",
      "Epoch: 71/200 Iteration: 10920 | Train loss: -363.45703\n",
      "Epoch: 71/200 Iteration: 10940 | Train loss: -256.43350\n",
      "Epoch: 71/200 Iteration: 10960 | Train loss: -363.48950\n",
      "Epoch: 71/200 Iteration: 10980 | Train loss: -438.86554\n",
      "Epoch: 71/200 Iteration: 11000 | Train loss: -498.84390\n",
      "Epoch: 72/200 Iteration: 11020 | Train loss: -400.50375\n",
      "Epoch: 72/200 Iteration: 11040 | Train loss: -425.55875\n",
      "Epoch: 72/200 Iteration: 11060 | Train loss: -465.63657\n",
      "Epoch: 72/200 Iteration: 11080 | Train loss: -380.83618\n",
      "Epoch: 72/200 Iteration: 11100 | Train loss: -353.47336\n",
      "Epoch: 72/200 Iteration: 11120 | Train loss: -434.58798\n",
      "Epoch: 72/200 Iteration: 11140 | Train loss: -427.06470\n",
      "Epoch: 72/200 Iteration: 11160 | Train loss: -441.24353\n",
      "Epoch: 73/200 Iteration: 11180 | Train loss: -309.67767\n",
      "Epoch: 73/200 Iteration: 11200 | Train loss: -538.88232\n",
      "Epoch: 73/200 Iteration: 11220 | Train loss: -357.45773\n",
      "Epoch: 73/200 Iteration: 11240 | Train loss: -379.51218\n",
      "Epoch: 73/200 Iteration: 11260 | Train loss: -497.11630\n",
      "Epoch: 73/200 Iteration: 11280 | Train loss: -416.10297\n",
      "Epoch: 73/200 Iteration: 11300 | Train loss: -493.33118\n",
      "Epoch: 74/200 Iteration: 11320 | Train loss: -533.83209\n",
      "Epoch: 74/200 Iteration: 11340 | Train loss: -382.47400\n",
      "Epoch: 74/200 Iteration: 11360 | Train loss: -370.36951\n",
      "Epoch: 74/200 Iteration: 11380 | Train loss: -328.71829\n",
      "Epoch: 74/200 Iteration: 11400 | Train loss: -504.92749\n",
      "Epoch: 74/200 Iteration: 11420 | Train loss: -302.39578\n",
      "Epoch: 74/200 Iteration: 11440 | Train loss: -443.86017\n",
      "Epoch: 74/200 Iteration: 11460 | Train loss: -448.69141\n",
      "Epoch: 75/200 Iteration: 11480 | Train loss: -457.39691\n",
      "Epoch: 75/200 Iteration: 11500 | Train loss: -552.33594\n",
      "Epoch: 75/200 Iteration: 11520 | Train loss: -543.19458\n",
      "Epoch: 75/200 Iteration: 11540 | Train loss: -387.83868\n",
      "Epoch: 75/200 Iteration: 11560 | Train loss: -271.08887\n",
      "Epoch: 75/200 Iteration: 11580 | Train loss: -403.88724\n",
      "Epoch: 75/200 Iteration: 11600 | Train loss: -470.29712\n",
      "Epoch: 75/200 Iteration: 11620 | Train loss: -543.14337\n",
      "Epoch: 76/200 Iteration: 11640 | Train loss: -430.88101\n",
      "Epoch: 76/200 Iteration: 11660 | Train loss: -450.26984\n",
      "Epoch: 76/200 Iteration: 11680 | Train loss: -483.18167\n",
      "Epoch: 76/200 Iteration: 11700 | Train loss: -394.78339\n",
      "Epoch: 76/200 Iteration: 11720 | Train loss: -367.90152\n",
      "Epoch: 76/200 Iteration: 11740 | Train loss: -450.21133\n",
      "Epoch: 76/200 Iteration: 11760 | Train loss: -448.56281\n",
      "Epoch: 76/200 Iteration: 11780 | Train loss: -461.37515\n",
      "Epoch: 77/200 Iteration: 11800 | Train loss: -323.29880\n",
      "Epoch: 77/200 Iteration: 11820 | Train loss: -562.82867\n",
      "Epoch: 77/200 Iteration: 11840 | Train loss: -384.91647\n",
      "Epoch: 77/200 Iteration: 11860 | Train loss: -378.09348\n",
      "Epoch: 77/200 Iteration: 11880 | Train loss: -536.36670\n",
      "Epoch: 77/200 Iteration: 11900 | Train loss: -445.16367\n",
      "Epoch: 77/200 Iteration: 11920 | Train loss: -534.36884\n",
      "Epoch: 78/200 Iteration: 11940 | Train loss: -567.55188\n",
      "Epoch: 78/200 Iteration: 11960 | Train loss: -403.83847\n",
      "Epoch: 78/200 Iteration: 11980 | Train loss: -407.25125\n",
      "Epoch: 78/200 Iteration: 12000 | Train loss: -346.63141\n",
      "Epoch: 78/200 Iteration: 12020 | Train loss: -522.67963\n",
      "Epoch: 78/200 Iteration: 12040 | Train loss: -308.72104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/200 Iteration: 12060 | Train loss: -472.04272\n",
      "Epoch: 78/200 Iteration: 12080 | Train loss: -449.42239\n",
      "Epoch: 79/200 Iteration: 12100 | Train loss: -484.46829\n",
      "Epoch: 79/200 Iteration: 12120 | Train loss: -574.09436\n",
      "Epoch: 79/200 Iteration: 12140 | Train loss: -563.89276\n",
      "Epoch: 79/200 Iteration: 12160 | Train loss: -407.59720\n",
      "Epoch: 79/200 Iteration: 12180 | Train loss: -281.75214\n",
      "Epoch: 79/200 Iteration: 12200 | Train loss: -414.62915\n",
      "Epoch: 79/200 Iteration: 12220 | Train loss: -514.49390\n",
      "Epoch: 79/200 Iteration: 12240 | Train loss: -551.14655\n",
      "Epoch: 80/200 Iteration: 12260 | Train loss: -457.55875\n",
      "Epoch: 80/200 Iteration: 12280 | Train loss: -482.75485\n",
      "Epoch: 80/200 Iteration: 12300 | Train loss: -493.55380\n",
      "Epoch: 80/200 Iteration: 12320 | Train loss: -415.52945\n",
      "Epoch: 80/200 Iteration: 12340 | Train loss: -392.02536\n",
      "Epoch: 80/200 Iteration: 12360 | Train loss: -471.07785\n",
      "Epoch: 80/200 Iteration: 12380 | Train loss: -470.03455\n",
      "Epoch: 80/200 Iteration: 12400 | Train loss: -483.56891\n",
      "Epoch: 81/200 Iteration: 12420 | Train loss: -332.80942\n",
      "Epoch: 81/200 Iteration: 12440 | Train loss: -596.66272\n",
      "Epoch: 81/200 Iteration: 12460 | Train loss: -403.96866\n",
      "Epoch: 81/200 Iteration: 12480 | Train loss: -417.69016\n",
      "Epoch: 81/200 Iteration: 12500 | Train loss: -559.72974\n",
      "Epoch: 81/200 Iteration: 12520 | Train loss: -460.13611\n",
      "Epoch: 81/200 Iteration: 12540 | Train loss: -574.42041\n",
      "Epoch: 82/200 Iteration: 12560 | Train loss: -600.87524\n",
      "Epoch: 82/200 Iteration: 12580 | Train loss: -442.58429\n",
      "Epoch: 82/200 Iteration: 12600 | Train loss: -432.71313\n",
      "Epoch: 82/200 Iteration: 12620 | Train loss: -354.61176\n",
      "Epoch: 82/200 Iteration: 12640 | Train loss: -553.39520\n",
      "Epoch: 82/200 Iteration: 12660 | Train loss: -330.13336\n",
      "Epoch: 82/200 Iteration: 12680 | Train loss: -486.79111\n",
      "Epoch: 82/200 Iteration: 12700 | Train loss: -493.98715\n",
      "Epoch: 83/200 Iteration: 12720 | Train loss: -503.40231\n",
      "Epoch: 83/200 Iteration: 12740 | Train loss: -612.41827\n",
      "Epoch: 83/200 Iteration: 12760 | Train loss: -587.34332\n",
      "Epoch: 83/200 Iteration: 12780 | Train loss: -424.03415\n",
      "Epoch: 83/200 Iteration: 12800 | Train loss: -295.08469\n",
      "Epoch: 83/200 Iteration: 12820 | Train loss: -441.01254\n",
      "Epoch: 83/200 Iteration: 12840 | Train loss: -529.71240\n",
      "Epoch: 83/200 Iteration: 12860 | Train loss: -591.77515\n",
      "Epoch: 84/200 Iteration: 12880 | Train loss: -496.07227\n",
      "Epoch: 84/200 Iteration: 12900 | Train loss: -491.99582\n",
      "Epoch: 84/200 Iteration: 12920 | Train loss: -533.07153\n",
      "Epoch: 84/200 Iteration: 12940 | Train loss: -429.74551\n",
      "Epoch: 84/200 Iteration: 12960 | Train loss: -414.93509\n",
      "Epoch: 84/200 Iteration: 12980 | Train loss: -505.94647\n",
      "Epoch: 84/200 Iteration: 13000 | Train loss: -495.76874\n",
      "Epoch: 84/200 Iteration: 13020 | Train loss: -506.02158\n",
      "Epoch: 85/200 Iteration: 13040 | Train loss: -359.13376\n",
      "Epoch: 85/200 Iteration: 13060 | Train loss: -654.18091\n",
      "Epoch: 85/200 Iteration: 13080 | Train loss: -401.48419\n",
      "Epoch: 85/200 Iteration: 13100 | Train loss: -444.46179\n",
      "Epoch: 85/200 Iteration: 13120 | Train loss: -588.13483\n",
      "Epoch: 85/200 Iteration: 13140 | Train loss: -485.81238\n",
      "Epoch: 85/200 Iteration: 13160 | Train loss: -591.44409\n",
      "Epoch: 86/200 Iteration: 13180 | Train loss: -621.35913\n",
      "Epoch: 86/200 Iteration: 13200 | Train loss: -444.16241\n",
      "Epoch: 86/200 Iteration: 13220 | Train loss: -459.30539\n",
      "Epoch: 86/200 Iteration: 13240 | Train loss: -375.02054\n",
      "Epoch: 86/200 Iteration: 13260 | Train loss: -581.87463\n",
      "Epoch: 86/200 Iteration: 13280 | Train loss: -339.99475\n",
      "Epoch: 86/200 Iteration: 13300 | Train loss: -514.11920\n",
      "Epoch: 86/200 Iteration: 13320 | Train loss: -510.44644\n",
      "Epoch: 87/200 Iteration: 13340 | Train loss: -539.04602\n",
      "Epoch: 87/200 Iteration: 13360 | Train loss: -642.20563\n",
      "Epoch: 87/200 Iteration: 13380 | Train loss: -598.45795\n",
      "Epoch: 87/200 Iteration: 13400 | Train loss: -452.25681\n",
      "Epoch: 87/200 Iteration: 13420 | Train loss: -306.84134\n",
      "Epoch: 87/200 Iteration: 13440 | Train loss: -460.19186\n",
      "Epoch: 87/200 Iteration: 13460 | Train loss: -551.48529\n",
      "Epoch: 87/200 Iteration: 13480 | Train loss: -621.13013\n",
      "Epoch: 88/200 Iteration: 13500 | Train loss: -488.39114\n",
      "Epoch: 88/200 Iteration: 13520 | Train loss: -516.09106\n",
      "Epoch: 88/200 Iteration: 13540 | Train loss: -550.27631\n",
      "Epoch: 88/200 Iteration: 13560 | Train loss: -453.47797\n",
      "Epoch: 88/200 Iteration: 13580 | Train loss: -434.90976\n",
      "Epoch: 88/200 Iteration: 13600 | Train loss: -520.97345\n",
      "Epoch: 88/200 Iteration: 13620 | Train loss: -515.12775\n",
      "Epoch: 88/200 Iteration: 13640 | Train loss: -536.22498\n",
      "Epoch: 89/200 Iteration: 13660 | Train loss: -383.28165\n",
      "Epoch: 89/200 Iteration: 13680 | Train loss: -655.41388\n",
      "Epoch: 89/200 Iteration: 13700 | Train loss: -431.49835\n",
      "Epoch: 89/200 Iteration: 13720 | Train loss: -461.18509\n",
      "Epoch: 89/200 Iteration: 13740 | Train loss: -609.62408\n",
      "Epoch: 89/200 Iteration: 13760 | Train loss: -485.34818\n",
      "Epoch: 89/200 Iteration: 13780 | Train loss: -620.27905\n",
      "Epoch: 90/200 Iteration: 13800 | Train loss: -652.23230\n",
      "Epoch: 90/200 Iteration: 13820 | Train loss: -459.69339\n",
      "Epoch: 90/200 Iteration: 13840 | Train loss: -451.73993\n",
      "Epoch: 90/200 Iteration: 13860 | Train loss: -388.61243\n",
      "Epoch: 90/200 Iteration: 13880 | Train loss: -589.27759\n",
      "Epoch: 90/200 Iteration: 13900 | Train loss: -361.45691\n",
      "Epoch: 90/200 Iteration: 13920 | Train loss: -537.09430\n",
      "Epoch: 90/200 Iteration: 13940 | Train loss: -522.60999\n",
      "Epoch: 91/200 Iteration: 13960 | Train loss: -566.71790\n",
      "Epoch: 91/200 Iteration: 13980 | Train loss: -686.24377\n",
      "Epoch: 91/200 Iteration: 14000 | Train loss: -639.64661\n",
      "Epoch: 91/200 Iteration: 14020 | Train loss: -464.75534\n",
      "Epoch: 91/200 Iteration: 14040 | Train loss: -335.60440\n",
      "Epoch: 91/200 Iteration: 14060 | Train loss: -473.99149\n",
      "Epoch: 91/200 Iteration: 14080 | Train loss: -579.16656\n",
      "Epoch: 91/200 Iteration: 14100 | Train loss: -649.86743\n",
      "Epoch: 92/200 Iteration: 14120 | Train loss: -532.43579\n",
      "Epoch: 92/200 Iteration: 14140 | Train loss: -537.55188\n",
      "Epoch: 92/200 Iteration: 14160 | Train loss: -575.04938\n",
      "Epoch: 92/200 Iteration: 14180 | Train loss: -464.86133\n",
      "Epoch: 92/200 Iteration: 14200 | Train loss: -452.21359\n",
      "Epoch: 92/200 Iteration: 14220 | Train loss: -556.11511\n",
      "Epoch: 92/200 Iteration: 14240 | Train loss: -533.05664\n",
      "Epoch: 92/200 Iteration: 14260 | Train loss: -573.03577\n",
      "Epoch: 93/200 Iteration: 14280 | Train loss: -388.74646\n",
      "Epoch: 93/200 Iteration: 14300 | Train loss: -698.36237\n",
      "Epoch: 93/200 Iteration: 14320 | Train loss: -471.87390\n",
      "Epoch: 93/200 Iteration: 14340 | Train loss: -482.85672\n",
      "Epoch: 93/200 Iteration: 14360 | Train loss: -621.83911\n",
      "Epoch: 93/200 Iteration: 14380 | Train loss: -510.14413\n",
      "Epoch: 93/200 Iteration: 14400 | Train loss: -641.27911\n",
      "Epoch: 94/200 Iteration: 14420 | Train loss: -678.11389\n",
      "Epoch: 94/200 Iteration: 14440 | Train loss: -495.61780\n",
      "Epoch: 94/200 Iteration: 14460 | Train loss: -472.14310\n",
      "Epoch: 94/200 Iteration: 14480 | Train loss: -398.09027\n",
      "Epoch: 94/200 Iteration: 14500 | Train loss: -638.17542\n",
      "Epoch: 94/200 Iteration: 14520 | Train loss: -376.20886\n",
      "Epoch: 94/200 Iteration: 14540 | Train loss: -559.00995\n",
      "Epoch: 94/200 Iteration: 14560 | Train loss: -564.20435\n",
      "Epoch: 95/200 Iteration: 14580 | Train loss: -572.63336\n",
      "Epoch: 95/200 Iteration: 14600 | Train loss: -705.45618\n",
      "Epoch: 95/200 Iteration: 14620 | Train loss: -668.24854\n",
      "Epoch: 95/200 Iteration: 14640 | Train loss: -485.54257\n",
      "Epoch: 95/200 Iteration: 14660 | Train loss: -352.14178\n",
      "Epoch: 95/200 Iteration: 14680 | Train loss: -517.84528\n",
      "Epoch: 95/200 Iteration: 14700 | Train loss: -592.77893\n",
      "Epoch: 95/200 Iteration: 14720 | Train loss: -686.65314\n",
      "Epoch: 96/200 Iteration: 14740 | Train loss: -549.70508\n",
      "Epoch: 96/200 Iteration: 14760 | Train loss: -567.61914\n",
      "Epoch: 96/200 Iteration: 14780 | Train loss: -602.64471\n",
      "Epoch: 96/200 Iteration: 14800 | Train loss: -502.23102\n",
      "Epoch: 96/200 Iteration: 14820 | Train loss: -462.54517\n",
      "Epoch: 96/200 Iteration: 14840 | Train loss: -546.49109\n",
      "Epoch: 96/200 Iteration: 14860 | Train loss: -566.51392\n",
      "Epoch: 96/200 Iteration: 14880 | Train loss: -588.43634\n",
      "Epoch: 97/200 Iteration: 14900 | Train loss: -390.61032\n",
      "Epoch: 97/200 Iteration: 14920 | Train loss: -725.66400\n",
      "Epoch: 97/200 Iteration: 14940 | Train loss: -474.55402\n",
      "Epoch: 97/200 Iteration: 14960 | Train loss: -500.01483\n",
      "Epoch: 97/200 Iteration: 14980 | Train loss: -670.58575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/200 Iteration: 15000 | Train loss: -540.07849\n",
      "Epoch: 97/200 Iteration: 15020 | Train loss: -682.94702\n",
      "Epoch: 98/200 Iteration: 15040 | Train loss: -712.49127\n",
      "Epoch: 98/200 Iteration: 15060 | Train loss: -512.57642\n",
      "Epoch: 98/200 Iteration: 15080 | Train loss: -507.34726\n",
      "Epoch: 98/200 Iteration: 15100 | Train loss: -431.35477\n",
      "Epoch: 98/200 Iteration: 15120 | Train loss: -643.26361\n",
      "Epoch: 98/200 Iteration: 15140 | Train loss: -404.47015\n",
      "Epoch: 98/200 Iteration: 15160 | Train loss: -570.63879\n",
      "Epoch: 98/200 Iteration: 15180 | Train loss: -583.71075\n",
      "Epoch: 99/200 Iteration: 15200 | Train loss: -599.40820\n",
      "Epoch: 99/200 Iteration: 15220 | Train loss: -747.99915\n",
      "Epoch: 99/200 Iteration: 15240 | Train loss: -709.73853\n",
      "Epoch: 99/200 Iteration: 15260 | Train loss: -497.85129\n",
      "Epoch: 99/200 Iteration: 15280 | Train loss: -367.35342\n",
      "Epoch: 99/200 Iteration: 15300 | Train loss: -533.15643\n",
      "Epoch: 99/200 Iteration: 15320 | Train loss: -620.99860\n",
      "Epoch: 99/200 Iteration: 15340 | Train loss: -704.52271\n",
      "Epoch: 100/200 Iteration: 15360 | Train loss: -559.40454\n",
      "Epoch: 100/200 Iteration: 15380 | Train loss: -590.23181\n",
      "Epoch: 100/200 Iteration: 15400 | Train loss: -620.50543\n",
      "Epoch: 100/200 Iteration: 15420 | Train loss: -516.37866\n",
      "Epoch: 100/200 Iteration: 15440 | Train loss: -503.18057\n",
      "Epoch: 100/200 Iteration: 15460 | Train loss: -588.93311\n",
      "Epoch: 100/200 Iteration: 15480 | Train loss: -591.84888\n",
      "Epoch: 100/200 Iteration: 15500 | Train loss: -622.75458\n",
      "Epoch: 101/200 Iteration: 15520 | Train loss: -426.51938\n",
      "Epoch: 101/200 Iteration: 15540 | Train loss: -752.09186\n",
      "Epoch: 101/200 Iteration: 15560 | Train loss: -485.58939\n",
      "Epoch: 101/200 Iteration: 15580 | Train loss: -515.06061\n",
      "Epoch: 101/200 Iteration: 15600 | Train loss: -677.86523\n",
      "Epoch: 101/200 Iteration: 15620 | Train loss: -563.11865\n",
      "Epoch: 101/200 Iteration: 15640 | Train loss: -704.65594\n",
      "Epoch: 102/200 Iteration: 15660 | Train loss: -747.17847\n",
      "Epoch: 102/200 Iteration: 15680 | Train loss: -522.09015\n",
      "Epoch: 102/200 Iteration: 15700 | Train loss: -508.55484\n",
      "Epoch: 102/200 Iteration: 15720 | Train loss: -428.77014\n",
      "Epoch: 102/200 Iteration: 15740 | Train loss: -686.85052\n",
      "Epoch: 102/200 Iteration: 15760 | Train loss: -407.18195\n",
      "Epoch: 102/200 Iteration: 15780 | Train loss: -612.08545\n",
      "Epoch: 102/200 Iteration: 15800 | Train loss: -609.15076\n",
      "Epoch: 103/200 Iteration: 15820 | Train loss: -626.42566\n",
      "Epoch: 103/200 Iteration: 15840 | Train loss: -767.70825\n",
      "Epoch: 103/200 Iteration: 15860 | Train loss: -724.47009\n",
      "Epoch: 103/200 Iteration: 15880 | Train loss: -528.29529\n",
      "Epoch: 103/200 Iteration: 15900 | Train loss: -365.84070\n",
      "Epoch: 103/200 Iteration: 15920 | Train loss: -550.84692\n",
      "Epoch: 103/200 Iteration: 15940 | Train loss: -631.96643\n",
      "Epoch: 103/200 Iteration: 15960 | Train loss: -709.92053\n",
      "Epoch: 104/200 Iteration: 15980 | Train loss: -597.31610\n",
      "Epoch: 104/200 Iteration: 16000 | Train loss: -625.67883\n",
      "Epoch: 104/200 Iteration: 16020 | Train loss: -662.09344\n",
      "Epoch: 104/200 Iteration: 16040 | Train loss: -534.56354\n",
      "Epoch: 104/200 Iteration: 16060 | Train loss: -525.15021\n",
      "Epoch: 104/200 Iteration: 16080 | Train loss: -623.05133\n",
      "Epoch: 104/200 Iteration: 16100 | Train loss: -627.81195\n",
      "Epoch: 104/200 Iteration: 16120 | Train loss: -634.11414\n",
      "Epoch: 105/200 Iteration: 16140 | Train loss: -435.07468\n",
      "Epoch: 105/200 Iteration: 16160 | Train loss: -783.01050\n",
      "Epoch: 105/200 Iteration: 16180 | Train loss: -512.73816\n",
      "Epoch: 105/200 Iteration: 16200 | Train loss: -536.24146\n",
      "Epoch: 105/200 Iteration: 16220 | Train loss: -707.38892\n",
      "Epoch: 105/200 Iteration: 16240 | Train loss: -582.89850\n",
      "Epoch: 105/200 Iteration: 16260 | Train loss: -708.88354\n",
      "Epoch: 106/200 Iteration: 16280 | Train loss: -761.35626\n",
      "Epoch: 106/200 Iteration: 16300 | Train loss: -550.14435\n",
      "Epoch: 106/200 Iteration: 16320 | Train loss: -541.24811\n",
      "Epoch: 106/200 Iteration: 16340 | Train loss: -469.84314\n",
      "Epoch: 106/200 Iteration: 16360 | Train loss: -712.51428\n",
      "Epoch: 106/200 Iteration: 16380 | Train loss: -428.75916\n",
      "Epoch: 106/200 Iteration: 16400 | Train loss: -638.62573\n",
      "Epoch: 106/200 Iteration: 16420 | Train loss: -644.44250\n",
      "Epoch: 107/200 Iteration: 16440 | Train loss: -640.10199\n",
      "Epoch: 107/200 Iteration: 16460 | Train loss: -803.86517\n",
      "Epoch: 107/200 Iteration: 16480 | Train loss: -769.39154\n",
      "Epoch: 107/200 Iteration: 16500 | Train loss: -546.16425\n",
      "Epoch: 107/200 Iteration: 16520 | Train loss: -386.72729\n",
      "Epoch: 107/200 Iteration: 16540 | Train loss: -584.11737\n",
      "Epoch: 107/200 Iteration: 16560 | Train loss: -676.50269\n",
      "Epoch: 107/200 Iteration: 16580 | Train loss: -758.37531\n",
      "Epoch: 108/200 Iteration: 16600 | Train loss: -614.41089\n",
      "Epoch: 108/200 Iteration: 16620 | Train loss: -663.48584\n",
      "Epoch: 108/200 Iteration: 16640 | Train loss: -686.36279\n",
      "Epoch: 108/200 Iteration: 16660 | Train loss: -563.99603\n",
      "Epoch: 108/200 Iteration: 16680 | Train loss: -541.51813\n",
      "Epoch: 108/200 Iteration: 16700 | Train loss: -634.52362\n",
      "Epoch: 108/200 Iteration: 16720 | Train loss: -644.65601\n",
      "Epoch: 108/200 Iteration: 16740 | Train loss: -668.82501\n",
      "Epoch: 109/200 Iteration: 16760 | Train loss: -450.44150\n",
      "Epoch: 109/200 Iteration: 16780 | Train loss: -817.88654\n",
      "Epoch: 109/200 Iteration: 16800 | Train loss: -545.24261\n",
      "Epoch: 109/200 Iteration: 16820 | Train loss: -554.75732\n",
      "Epoch: 109/200 Iteration: 16840 | Train loss: -747.61798\n",
      "Epoch: 109/200 Iteration: 16860 | Train loss: -599.79077\n",
      "Epoch: 109/200 Iteration: 16880 | Train loss: -765.30920\n",
      "Epoch: 110/200 Iteration: 16900 | Train loss: -772.67755\n",
      "Epoch: 110/200 Iteration: 16920 | Train loss: -560.62579\n",
      "Epoch: 110/200 Iteration: 16940 | Train loss: -571.00940\n",
      "Epoch: 110/200 Iteration: 16960 | Train loss: -485.75418\n",
      "Epoch: 110/200 Iteration: 16980 | Train loss: -746.83752\n",
      "Epoch: 110/200 Iteration: 17000 | Train loss: -427.30640\n",
      "Epoch: 110/200 Iteration: 17020 | Train loss: -662.98456\n",
      "Epoch: 110/200 Iteration: 17040 | Train loss: -650.38147\n",
      "Epoch: 111/200 Iteration: 17060 | Train loss: -678.35748\n",
      "Epoch: 111/200 Iteration: 17080 | Train loss: -809.67407\n",
      "Epoch: 111/200 Iteration: 17100 | Train loss: -782.85931\n",
      "Epoch: 111/200 Iteration: 17120 | Train loss: -568.09088\n",
      "Epoch: 111/200 Iteration: 17140 | Train loss: -409.56915\n",
      "Epoch: 111/200 Iteration: 17160 | Train loss: -611.08942\n",
      "Epoch: 111/200 Iteration: 17180 | Train loss: -719.45349\n",
      "Epoch: 111/200 Iteration: 17200 | Train loss: -801.72107\n",
      "Epoch: 112/200 Iteration: 17220 | Train loss: -632.34155\n",
      "Epoch: 112/200 Iteration: 17240 | Train loss: -659.93805\n",
      "Epoch: 112/200 Iteration: 17260 | Train loss: -704.84955\n",
      "Epoch: 112/200 Iteration: 17280 | Train loss: -574.10992\n",
      "Epoch: 112/200 Iteration: 17300 | Train loss: -569.69104\n",
      "Epoch: 112/200 Iteration: 17320 | Train loss: -657.04797\n",
      "Epoch: 112/200 Iteration: 17340 | Train loss: -665.35815\n",
      "Epoch: 112/200 Iteration: 17360 | Train loss: -689.26929\n",
      "Epoch: 113/200 Iteration: 17380 | Train loss: -471.64560\n",
      "Epoch: 113/200 Iteration: 17400 | Train loss: -858.39099\n",
      "Epoch: 113/200 Iteration: 17420 | Train loss: -547.59418\n",
      "Epoch: 113/200 Iteration: 17440 | Train loss: -574.16931\n",
      "Epoch: 113/200 Iteration: 17460 | Train loss: -776.14893\n",
      "Epoch: 113/200 Iteration: 17480 | Train loss: -622.39154\n",
      "Epoch: 113/200 Iteration: 17500 | Train loss: -763.33472\n",
      "Epoch: 114/200 Iteration: 17520 | Train loss: -816.10687\n",
      "Epoch: 114/200 Iteration: 17540 | Train loss: -578.55713\n",
      "Epoch: 114/200 Iteration: 17560 | Train loss: -574.98767\n",
      "Epoch: 114/200 Iteration: 17580 | Train loss: -505.99042\n",
      "Epoch: 114/200 Iteration: 17600 | Train loss: -758.95795\n",
      "Epoch: 114/200 Iteration: 17620 | Train loss: -464.62350\n",
      "Epoch: 114/200 Iteration: 17640 | Train loss: -662.29053\n",
      "Epoch: 114/200 Iteration: 17660 | Train loss: -688.05292\n",
      "Epoch: 115/200 Iteration: 17680 | Train loss: -705.86041\n",
      "Epoch: 115/200 Iteration: 17700 | Train loss: -846.86047\n",
      "Epoch: 115/200 Iteration: 17720 | Train loss: -788.23846\n",
      "Epoch: 115/200 Iteration: 17740 | Train loss: -597.06165\n",
      "Epoch: 115/200 Iteration: 17760 | Train loss: -413.17398\n",
      "Epoch: 115/200 Iteration: 17780 | Train loss: -612.46021\n",
      "Epoch: 115/200 Iteration: 17800 | Train loss: -730.55109\n",
      "Epoch: 115/200 Iteration: 17820 | Train loss: -835.84784\n",
      "Epoch: 116/200 Iteration: 17840 | Train loss: -670.54858\n",
      "Epoch: 116/200 Iteration: 17860 | Train loss: -677.97388\n",
      "Epoch: 116/200 Iteration: 17880 | Train loss: -743.19391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116/200 Iteration: 17900 | Train loss: -598.86212\n",
      "Epoch: 116/200 Iteration: 17920 | Train loss: -570.36224\n",
      "Epoch: 116/200 Iteration: 17940 | Train loss: -696.55029\n",
      "Epoch: 116/200 Iteration: 17960 | Train loss: -701.47937\n",
      "Epoch: 116/200 Iteration: 17980 | Train loss: -715.45264\n",
      "Epoch: 117/200 Iteration: 18000 | Train loss: -496.88962\n",
      "Epoch: 117/200 Iteration: 18020 | Train loss: -848.84564\n",
      "Epoch: 117/200 Iteration: 18040 | Train loss: -587.35052\n",
      "Epoch: 117/200 Iteration: 18060 | Train loss: -604.29382\n",
      "Epoch: 117/200 Iteration: 18080 | Train loss: -779.87439\n",
      "Epoch: 117/200 Iteration: 18100 | Train loss: -632.20349\n",
      "Epoch: 117/200 Iteration: 18120 | Train loss: -806.29626\n",
      "Epoch: 118/200 Iteration: 18140 | Train loss: -842.49969\n",
      "Epoch: 118/200 Iteration: 18160 | Train loss: -591.38593\n",
      "Epoch: 118/200 Iteration: 18180 | Train loss: -613.04584\n",
      "Epoch: 118/200 Iteration: 18200 | Train loss: -510.23962\n",
      "Epoch: 118/200 Iteration: 18220 | Train loss: -784.87531\n",
      "Epoch: 118/200 Iteration: 18240 | Train loss: -476.18976\n",
      "Epoch: 118/200 Iteration: 18260 | Train loss: -707.34155\n",
      "Epoch: 118/200 Iteration: 18280 | Train loss: -684.20905\n",
      "Epoch: 119/200 Iteration: 18300 | Train loss: -712.27313\n",
      "Epoch: 119/200 Iteration: 18320 | Train loss: -875.87274\n",
      "Epoch: 119/200 Iteration: 18340 | Train loss: -844.37860\n",
      "Epoch: 119/200 Iteration: 18360 | Train loss: -608.91284\n",
      "Epoch: 119/200 Iteration: 18380 | Train loss: -450.49850\n",
      "Epoch: 119/200 Iteration: 18400 | Train loss: -640.39447\n",
      "Epoch: 119/200 Iteration: 18420 | Train loss: -750.23438\n",
      "Epoch: 119/200 Iteration: 18440 | Train loss: -832.64984\n",
      "Epoch: 120/200 Iteration: 18460 | Train loss: -675.29065\n",
      "Epoch: 120/200 Iteration: 18480 | Train loss: -716.71710\n",
      "Epoch: 120/200 Iteration: 18500 | Train loss: -738.42242\n",
      "Epoch: 120/200 Iteration: 18520 | Train loss: -610.85529\n",
      "Epoch: 120/200 Iteration: 18540 | Train loss: -605.24115\n",
      "Epoch: 120/200 Iteration: 18560 | Train loss: -688.70990\n",
      "Epoch: 120/200 Iteration: 18580 | Train loss: -704.98798\n",
      "Epoch: 120/200 Iteration: 18600 | Train loss: -730.25751\n",
      "Epoch: 121/200 Iteration: 18620 | Train loss: -498.07657\n",
      "Epoch: 121/200 Iteration: 18640 | Train loss: -915.76111\n",
      "Epoch: 121/200 Iteration: 18660 | Train loss: -606.23120\n",
      "Epoch: 121/200 Iteration: 18680 | Train loss: -628.87677\n",
      "Epoch: 121/200 Iteration: 18700 | Train loss: -812.54156\n",
      "Epoch: 121/200 Iteration: 18720 | Train loss: -684.76471\n",
      "Epoch: 121/200 Iteration: 18740 | Train loss: -814.09045\n",
      "Epoch: 122/200 Iteration: 18760 | Train loss: -884.23987\n",
      "Epoch: 122/200 Iteration: 18780 | Train loss: -622.55170\n",
      "Epoch: 122/200 Iteration: 18800 | Train loss: -631.72906\n",
      "Epoch: 122/200 Iteration: 18820 | Train loss: -530.10480\n",
      "Epoch: 122/200 Iteration: 18840 | Train loss: -806.56689\n",
      "Epoch: 122/200 Iteration: 18860 | Train loss: -484.45386\n",
      "Epoch: 122/200 Iteration: 18880 | Train loss: -716.90997\n",
      "Epoch: 122/200 Iteration: 18900 | Train loss: -723.34857\n",
      "Epoch: 123/200 Iteration: 18920 | Train loss: -757.54437\n",
      "Epoch: 123/200 Iteration: 18940 | Train loss: -907.57123\n",
      "Epoch: 123/200 Iteration: 18960 | Train loss: -850.14893\n",
      "Epoch: 123/200 Iteration: 18980 | Train loss: -632.35083\n",
      "Epoch: 123/200 Iteration: 19000 | Train loss: -453.12836\n",
      "Epoch: 123/200 Iteration: 19020 | Train loss: -648.27460\n",
      "Epoch: 123/200 Iteration: 19040 | Train loss: -764.17859\n",
      "Epoch: 123/200 Iteration: 19060 | Train loss: -876.30939\n",
      "Epoch: 124/200 Iteration: 19080 | Train loss: -713.62732\n",
      "Epoch: 124/200 Iteration: 19100 | Train loss: -724.08167\n",
      "Epoch: 124/200 Iteration: 19120 | Train loss: -800.28998\n",
      "Epoch: 124/200 Iteration: 19140 | Train loss: -640.93018\n",
      "Epoch: 124/200 Iteration: 19160 | Train loss: -607.99475\n",
      "Epoch: 124/200 Iteration: 19180 | Train loss: -748.09875\n",
      "Epoch: 124/200 Iteration: 19200 | Train loss: -744.10358\n",
      "Epoch: 124/200 Iteration: 19220 | Train loss: -764.03900\n",
      "Epoch: 125/200 Iteration: 19240 | Train loss: -533.86768\n",
      "Epoch: 125/200 Iteration: 19260 | Train loss: -934.69604\n",
      "Epoch: 125/200 Iteration: 19280 | Train loss: -629.46344\n",
      "Epoch: 125/200 Iteration: 19300 | Train loss: -659.91040\n",
      "Epoch: 125/200 Iteration: 19320 | Train loss: -853.19110\n",
      "Epoch: 125/200 Iteration: 19340 | Train loss: -692.32092\n",
      "Epoch: 125/200 Iteration: 19360 | Train loss: -862.99860\n",
      "Epoch: 126/200 Iteration: 19380 | Train loss: -935.59900\n",
      "Epoch: 126/200 Iteration: 19400 | Train loss: -619.56195\n",
      "Epoch: 126/200 Iteration: 19420 | Train loss: -660.10944\n",
      "Epoch: 126/200 Iteration: 19440 | Train loss: -552.92786\n",
      "Epoch: 126/200 Iteration: 19460 | Train loss: -838.37341\n",
      "Epoch: 126/200 Iteration: 19480 | Train loss: -477.22672\n",
      "Epoch: 126/200 Iteration: 19500 | Train loss: -746.24420\n",
      "Epoch: 126/200 Iteration: 19520 | Train loss: -756.53076\n",
      "Epoch: 127/200 Iteration: 19540 | Train loss: -753.69507\n",
      "Epoch: 127/200 Iteration: 19560 | Train loss: -967.94861\n",
      "Epoch: 127/200 Iteration: 19580 | Train loss: -873.40063\n",
      "Epoch: 127/200 Iteration: 19600 | Train loss: -663.76282\n",
      "Epoch: 127/200 Iteration: 19620 | Train loss: -468.16376\n",
      "Epoch: 127/200 Iteration: 19640 | Train loss: -706.59265\n",
      "Epoch: 127/200 Iteration: 19660 | Train loss: -774.13361\n",
      "Epoch: 127/200 Iteration: 19680 | Train loss: -915.14233\n",
      "Epoch: 128/200 Iteration: 19700 | Train loss: -726.65356\n",
      "Epoch: 128/200 Iteration: 19720 | Train loss: -781.07245\n",
      "Epoch: 128/200 Iteration: 19740 | Train loss: -791.11377\n",
      "Epoch: 128/200 Iteration: 19760 | Train loss: -648.07861\n",
      "Epoch: 128/200 Iteration: 19780 | Train loss: -628.50488\n",
      "Epoch: 128/200 Iteration: 19800 | Train loss: -766.81940\n",
      "Epoch: 128/200 Iteration: 19820 | Train loss: -747.60632\n",
      "Epoch: 128/200 Iteration: 19840 | Train loss: -778.17700\n",
      "Epoch: 129/200 Iteration: 19860 | Train loss: -533.88910\n",
      "Epoch: 129/200 Iteration: 19880 | Train loss: -960.42468\n",
      "Epoch: 129/200 Iteration: 19900 | Train loss: -639.80072\n",
      "Epoch: 129/200 Iteration: 19920 | Train loss: -662.48169\n",
      "Epoch: 129/200 Iteration: 19940 | Train loss: -873.73376\n",
      "Epoch: 129/200 Iteration: 19960 | Train loss: -700.23865\n",
      "Epoch: 129/200 Iteration: 19980 | Train loss: -884.89990\n",
      "Epoch: 130/200 Iteration: 20000 | Train loss: -946.03076\n",
      "Epoch: 130/200 Iteration: 20020 | Train loss: -670.58234\n",
      "Epoch: 130/200 Iteration: 20040 | Train loss: -684.55096\n",
      "Epoch: 130/200 Iteration: 20060 | Train loss: -575.79443\n",
      "Epoch: 130/200 Iteration: 20080 | Train loss: -881.86798\n",
      "Epoch: 130/200 Iteration: 20100 | Train loss: -512.66107\n",
      "Epoch: 130/200 Iteration: 20120 | Train loss: -779.01672\n",
      "Epoch: 130/200 Iteration: 20140 | Train loss: -761.91907\n",
      "Epoch: 131/200 Iteration: 20160 | Train loss: -811.64795\n",
      "Epoch: 131/200 Iteration: 20180 | Train loss: -987.79797\n",
      "Epoch: 131/200 Iteration: 20200 | Train loss: -910.18518\n",
      "Epoch: 131/200 Iteration: 20220 | Train loss: -670.93384\n",
      "Epoch: 131/200 Iteration: 20240 | Train loss: -474.28241\n",
      "Epoch: 131/200 Iteration: 20260 | Train loss: -679.19244\n",
      "Epoch: 131/200 Iteration: 20280 | Train loss: -812.37201\n",
      "Epoch: 131/200 Iteration: 20300 | Train loss: -924.91913\n",
      "Epoch: 132/200 Iteration: 20320 | Train loss: -770.05627\n",
      "Epoch: 132/200 Iteration: 20340 | Train loss: -767.93909\n",
      "Epoch: 132/200 Iteration: 20360 | Train loss: -817.11523\n",
      "Epoch: 132/200 Iteration: 20380 | Train loss: -685.78003\n",
      "Epoch: 132/200 Iteration: 20400 | Train loss: -656.11737\n",
      "Epoch: 132/200 Iteration: 20420 | Train loss: -771.53186\n",
      "Epoch: 132/200 Iteration: 20440 | Train loss: -801.32593\n",
      "Epoch: 132/200 Iteration: 20460 | Train loss: -808.59607\n",
      "Epoch: 133/200 Iteration: 20480 | Train loss: -566.19971\n",
      "Epoch: 133/200 Iteration: 20500 | Train loss: -990.39178\n",
      "Epoch: 133/200 Iteration: 20520 | Train loss: -659.37555\n",
      "Epoch: 133/200 Iteration: 20540 | Train loss: -664.38849\n",
      "Epoch: 133/200 Iteration: 20560 | Train loss: -903.47516\n",
      "Epoch: 133/200 Iteration: 20580 | Train loss: -728.75116\n",
      "Epoch: 133/200 Iteration: 20600 | Train loss: -916.37396\n",
      "Epoch: 134/200 Iteration: 20620 | Train loss: -974.17487\n",
      "Epoch: 134/200 Iteration: 20640 | Train loss: -704.86530\n",
      "Epoch: 134/200 Iteration: 20660 | Train loss: -676.38275\n",
      "Epoch: 134/200 Iteration: 20680 | Train loss: -579.84906\n",
      "Epoch: 134/200 Iteration: 20700 | Train loss: -877.18524\n",
      "Epoch: 134/200 Iteration: 20720 | Train loss: -521.04578\n",
      "Epoch: 134/200 Iteration: 20740 | Train loss: -797.87939\n",
      "Epoch: 134/200 Iteration: 20760 | Train loss: -789.10681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135/200 Iteration: 20780 | Train loss: -805.47461\n",
      "Epoch: 135/200 Iteration: 20800 | Train loss: -1005.35205\n",
      "Epoch: 135/200 Iteration: 20820 | Train loss: -950.52393\n",
      "Epoch: 135/200 Iteration: 20840 | Train loss: -699.00720\n",
      "Epoch: 135/200 Iteration: 20860 | Train loss: -473.12469\n",
      "Epoch: 135/200 Iteration: 20880 | Train loss: -716.05420\n",
      "Epoch: 135/200 Iteration: 20900 | Train loss: -853.00763\n",
      "Epoch: 135/200 Iteration: 20920 | Train loss: -984.74249\n",
      "Epoch: 136/200 Iteration: 20940 | Train loss: -778.57794\n",
      "Epoch: 136/200 Iteration: 20960 | Train loss: -805.37689\n",
      "Epoch: 136/200 Iteration: 20980 | Train loss: -840.93237\n",
      "Epoch: 136/200 Iteration: 21000 | Train loss: -707.35883\n",
      "Epoch: 136/200 Iteration: 21020 | Train loss: -675.44141\n",
      "Epoch: 136/200 Iteration: 21040 | Train loss: -811.01672\n",
      "Epoch: 136/200 Iteration: 21060 | Train loss: -801.64929\n",
      "Epoch: 136/200 Iteration: 21080 | Train loss: -831.47302\n",
      "Epoch: 137/200 Iteration: 21100 | Train loss: -568.95062\n",
      "Epoch: 137/200 Iteration: 21120 | Train loss: -1032.21033\n",
      "Epoch: 137/200 Iteration: 21140 | Train loss: -718.40533\n",
      "Epoch: 137/200 Iteration: 21160 | Train loss: -697.26593\n",
      "Epoch: 137/200 Iteration: 21180 | Train loss: -932.74146\n",
      "Epoch: 137/200 Iteration: 21200 | Train loss: -745.62341\n",
      "Epoch: 137/200 Iteration: 21220 | Train loss: -938.52753\n",
      "Epoch: 138/200 Iteration: 21240 | Train loss: -974.78546\n",
      "Epoch: 138/200 Iteration: 21260 | Train loss: -694.05664\n",
      "Epoch: 138/200 Iteration: 21280 | Train loss: -703.98993\n",
      "Epoch: 138/200 Iteration: 21300 | Train loss: -604.47736\n",
      "Epoch: 138/200 Iteration: 21320 | Train loss: -930.61737\n",
      "Epoch: 138/200 Iteration: 21340 | Train loss: -546.70490\n",
      "Epoch: 138/200 Iteration: 21360 | Train loss: -829.73218\n",
      "Epoch: 138/200 Iteration: 21380 | Train loss: -794.38873\n",
      "Epoch: 139/200 Iteration: 21400 | Train loss: -845.15686\n",
      "Epoch: 139/200 Iteration: 21420 | Train loss: -1033.36072\n",
      "Epoch: 139/200 Iteration: 21440 | Train loss: -985.76404\n",
      "Epoch: 139/200 Iteration: 21460 | Train loss: -720.47845\n",
      "Epoch: 139/200 Iteration: 21480 | Train loss: -489.33359\n",
      "Epoch: 139/200 Iteration: 21500 | Train loss: -732.01013\n",
      "Epoch: 139/200 Iteration: 21520 | Train loss: -870.70935\n",
      "Epoch: 139/200 Iteration: 21540 | Train loss: -982.41016\n",
      "Epoch: 140/200 Iteration: 21560 | Train loss: -805.71265\n",
      "Epoch: 140/200 Iteration: 21580 | Train loss: -818.28943\n",
      "Epoch: 140/200 Iteration: 21600 | Train loss: -859.10083\n",
      "Epoch: 140/200 Iteration: 21620 | Train loss: -731.90179\n",
      "Epoch: 140/200 Iteration: 21640 | Train loss: -696.48743\n",
      "Epoch: 140/200 Iteration: 21660 | Train loss: -835.89752\n",
      "Epoch: 140/200 Iteration: 21680 | Train loss: -869.04468\n",
      "Epoch: 140/200 Iteration: 21700 | Train loss: -845.12909\n",
      "Epoch: 141/200 Iteration: 21720 | Train loss: -587.99054\n",
      "Epoch: 141/200 Iteration: 21740 | Train loss: -1058.97412\n",
      "Epoch: 141/200 Iteration: 21760 | Train loss: -691.96155\n",
      "Epoch: 141/200 Iteration: 21780 | Train loss: -734.87030\n",
      "Epoch: 141/200 Iteration: 21800 | Train loss: -946.73431\n",
      "Epoch: 141/200 Iteration: 21820 | Train loss: -800.49854\n",
      "Epoch: 141/200 Iteration: 21840 | Train loss: -970.52808\n",
      "Epoch: 142/200 Iteration: 21860 | Train loss: -1030.00159\n",
      "Epoch: 142/200 Iteration: 21880 | Train loss: -709.80579\n",
      "Epoch: 142/200 Iteration: 21900 | Train loss: -724.59003\n",
      "Epoch: 142/200 Iteration: 21920 | Train loss: -639.27869\n",
      "Epoch: 142/200 Iteration: 21940 | Train loss: -932.08795\n",
      "Epoch: 142/200 Iteration: 21960 | Train loss: -572.36548\n",
      "Epoch: 142/200 Iteration: 21980 | Train loss: -837.81763\n",
      "Epoch: 142/200 Iteration: 22000 | Train loss: -842.93329\n",
      "Epoch: 143/200 Iteration: 22020 | Train loss: -870.17780\n",
      "Epoch: 143/200 Iteration: 22040 | Train loss: -1041.14160\n",
      "Epoch: 143/200 Iteration: 22060 | Train loss: -1010.75201\n",
      "Epoch: 143/200 Iteration: 22080 | Train loss: -737.49219\n",
      "Epoch: 143/200 Iteration: 22100 | Train loss: -515.74475\n",
      "Epoch: 143/200 Iteration: 22120 | Train loss: -715.45331\n",
      "Epoch: 143/200 Iteration: 22140 | Train loss: -907.52899\n",
      "Epoch: 143/200 Iteration: 22160 | Train loss: -1003.45331\n",
      "Epoch: 144/200 Iteration: 22180 | Train loss: -841.81976\n",
      "Epoch: 144/200 Iteration: 22200 | Train loss: -847.48749\n",
      "Epoch: 144/200 Iteration: 22220 | Train loss: -898.67212\n",
      "Epoch: 144/200 Iteration: 22240 | Train loss: -739.14142\n",
      "Epoch: 144/200 Iteration: 22260 | Train loss: -717.64740\n",
      "Epoch: 144/200 Iteration: 22280 | Train loss: -861.62689\n",
      "Epoch: 144/200 Iteration: 22300 | Train loss: -866.82471\n",
      "Epoch: 144/200 Iteration: 22320 | Train loss: -881.50409\n",
      "Epoch: 145/200 Iteration: 22340 | Train loss: -627.41412\n",
      "Epoch: 145/200 Iteration: 22360 | Train loss: -1075.38489\n",
      "Epoch: 145/200 Iteration: 22380 | Train loss: -708.20001\n",
      "Epoch: 145/200 Iteration: 22400 | Train loss: -732.48547\n",
      "Epoch: 145/200 Iteration: 22420 | Train loss: -978.30493\n",
      "Epoch: 145/200 Iteration: 22440 | Train loss: -788.03729\n",
      "Epoch: 145/200 Iteration: 22460 | Train loss: -1009.28967\n",
      "Epoch: 146/200 Iteration: 22480 | Train loss: -1042.45166\n",
      "Epoch: 146/200 Iteration: 22500 | Train loss: -730.56299\n",
      "Epoch: 146/200 Iteration: 22520 | Train loss: -741.00195\n",
      "Epoch: 146/200 Iteration: 22540 | Train loss: -632.18652\n",
      "Epoch: 146/200 Iteration: 22560 | Train loss: -970.54266\n",
      "Epoch: 146/200 Iteration: 22580 | Train loss: -588.30957\n",
      "Epoch: 146/200 Iteration: 22600 | Train loss: -853.29541\n",
      "Epoch: 146/200 Iteration: 22620 | Train loss: -836.00665\n",
      "Epoch: 147/200 Iteration: 22640 | Train loss: -898.51080\n",
      "Epoch: 147/200 Iteration: 22660 | Train loss: -1090.35400\n",
      "Epoch: 147/200 Iteration: 22680 | Train loss: -1036.43518\n",
      "Epoch: 147/200 Iteration: 22700 | Train loss: -753.63263\n",
      "Epoch: 147/200 Iteration: 22720 | Train loss: -530.99475\n",
      "Epoch: 147/200 Iteration: 22740 | Train loss: -791.86359\n",
      "Epoch: 147/200 Iteration: 22760 | Train loss: -939.09955\n",
      "Epoch: 147/200 Iteration: 22780 | Train loss: -1033.16504\n",
      "Epoch: 148/200 Iteration: 22800 | Train loss: -850.65289\n",
      "Epoch: 148/200 Iteration: 22820 | Train loss: -883.20282\n",
      "Epoch: 148/200 Iteration: 22840 | Train loss: -937.10919\n",
      "Epoch: 148/200 Iteration: 22860 | Train loss: -750.21814\n",
      "Epoch: 148/200 Iteration: 22880 | Train loss: -704.92346\n",
      "Epoch: 148/200 Iteration: 22900 | Train loss: -894.10913\n",
      "Epoch: 148/200 Iteration: 22920 | Train loss: -907.08618\n",
      "Epoch: 148/200 Iteration: 22940 | Train loss: -912.61450\n",
      "Epoch: 149/200 Iteration: 22960 | Train loss: -635.31366\n",
      "Epoch: 149/200 Iteration: 22980 | Train loss: -1115.52637\n",
      "Epoch: 149/200 Iteration: 23000 | Train loss: -730.84747\n",
      "Epoch: 149/200 Iteration: 23020 | Train loss: -774.86780\n",
      "Epoch: 149/200 Iteration: 23040 | Train loss: -1059.73645\n",
      "Epoch: 149/200 Iteration: 23060 | Train loss: -813.78009\n",
      "Epoch: 149/200 Iteration: 23080 | Train loss: -1009.11627\n",
      "Epoch: 150/200 Iteration: 23100 | Train loss: -1083.23755\n",
      "Epoch: 150/200 Iteration: 23120 | Train loss: -771.65149\n",
      "Epoch: 150/200 Iteration: 23140 | Train loss: -760.04797\n",
      "Epoch: 150/200 Iteration: 23160 | Train loss: -683.64032\n",
      "Epoch: 150/200 Iteration: 23180 | Train loss: -982.56921\n",
      "Epoch: 150/200 Iteration: 23200 | Train loss: -609.08881\n",
      "Epoch: 150/200 Iteration: 23220 | Train loss: -884.76721\n",
      "Epoch: 150/200 Iteration: 23240 | Train loss: -907.66107\n",
      "Epoch: 151/200 Iteration: 23260 | Train loss: -933.61237\n",
      "Epoch: 151/200 Iteration: 23280 | Train loss: -1118.28369\n",
      "Epoch: 151/200 Iteration: 23300 | Train loss: -1051.58875\n",
      "Epoch: 151/200 Iteration: 23320 | Train loss: -776.17218\n",
      "Epoch: 151/200 Iteration: 23340 | Train loss: -541.75073\n",
      "Epoch: 151/200 Iteration: 23360 | Train loss: -796.23560\n",
      "Epoch: 151/200 Iteration: 23380 | Train loss: -969.39197\n",
      "Epoch: 151/200 Iteration: 23400 | Train loss: -1042.19495\n",
      "Epoch: 152/200 Iteration: 23420 | Train loss: -861.15100\n",
      "Epoch: 152/200 Iteration: 23440 | Train loss: -897.54596\n",
      "Epoch: 152/200 Iteration: 23460 | Train loss: -965.14233\n",
      "Epoch: 152/200 Iteration: 23480 | Train loss: -786.36157\n",
      "Epoch: 152/200 Iteration: 23500 | Train loss: -774.24854\n",
      "Epoch: 152/200 Iteration: 23520 | Train loss: -889.56555\n",
      "Epoch: 152/200 Iteration: 23540 | Train loss: -883.67639\n",
      "Epoch: 152/200 Iteration: 23560 | Train loss: -952.94501\n",
      "Epoch: 153/200 Iteration: 23580 | Train loss: -636.08380\n",
      "Epoch: 153/200 Iteration: 23600 | Train loss: -1161.69080\n",
      "Epoch: 153/200 Iteration: 23620 | Train loss: -743.65375\n",
      "Epoch: 153/200 Iteration: 23640 | Train loss: -779.59967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153/200 Iteration: 23660 | Train loss: -1053.26440\n",
      "Epoch: 153/200 Iteration: 23680 | Train loss: -803.59607\n",
      "Epoch: 153/200 Iteration: 23700 | Train loss: -1063.66589\n",
      "Epoch: 154/200 Iteration: 23720 | Train loss: -1122.58496\n",
      "Epoch: 154/200 Iteration: 23740 | Train loss: -799.53943\n",
      "Epoch: 154/200 Iteration: 23760 | Train loss: -788.27515\n",
      "Epoch: 154/200 Iteration: 23780 | Train loss: -702.53766\n",
      "Epoch: 154/200 Iteration: 23800 | Train loss: -1039.25452\n",
      "Epoch: 154/200 Iteration: 23820 | Train loss: -606.23755\n",
      "Epoch: 154/200 Iteration: 23840 | Train loss: -906.50079\n",
      "Epoch: 154/200 Iteration: 23860 | Train loss: -910.06702\n",
      "Epoch: 155/200 Iteration: 23880 | Train loss: -957.58765\n",
      "Epoch: 155/200 Iteration: 23900 | Train loss: -1115.42810\n",
      "Epoch: 155/200 Iteration: 23920 | Train loss: -1107.80371\n",
      "Epoch: 155/200 Iteration: 23940 | Train loss: -788.22211\n",
      "Epoch: 155/200 Iteration: 23960 | Train loss: -557.32623\n",
      "Epoch: 155/200 Iteration: 23980 | Train loss: -857.82935\n",
      "Epoch: 155/200 Iteration: 24000 | Train loss: -980.45044\n",
      "Epoch: 155/200 Iteration: 24020 | Train loss: -1076.90405\n",
      "Epoch: 156/200 Iteration: 24040 | Train loss: -892.39294\n",
      "Epoch: 156/200 Iteration: 24060 | Train loss: -920.92114\n",
      "Epoch: 156/200 Iteration: 24080 | Train loss: -940.74939\n",
      "Epoch: 156/200 Iteration: 24100 | Train loss: -790.10596\n",
      "Epoch: 156/200 Iteration: 24120 | Train loss: -785.15485\n",
      "Epoch: 156/200 Iteration: 24140 | Train loss: -926.67462\n",
      "Epoch: 156/200 Iteration: 24160 | Train loss: -883.82898\n",
      "Epoch: 156/200 Iteration: 24180 | Train loss: -961.58594\n",
      "Epoch: 157/200 Iteration: 24200 | Train loss: -642.17462\n",
      "Epoch: 157/200 Iteration: 24220 | Train loss: -1159.54761\n",
      "Epoch: 157/200 Iteration: 24240 | Train loss: -773.12329\n",
      "Epoch: 157/200 Iteration: 24260 | Train loss: -790.00220\n",
      "Epoch: 157/200 Iteration: 24280 | Train loss: -1097.62708\n",
      "Epoch: 157/200 Iteration: 24300 | Train loss: -872.60852\n",
      "Epoch: 157/200 Iteration: 24320 | Train loss: -1076.39209\n",
      "Epoch: 158/200 Iteration: 24340 | Train loss: -1141.05603\n",
      "Epoch: 158/200 Iteration: 24360 | Train loss: -802.57892\n",
      "Epoch: 158/200 Iteration: 24380 | Train loss: -803.44922\n",
      "Epoch: 158/200 Iteration: 24400 | Train loss: -677.22742\n",
      "Epoch: 158/200 Iteration: 24420 | Train loss: -1046.07068\n",
      "Epoch: 158/200 Iteration: 24440 | Train loss: -643.10358\n",
      "Epoch: 158/200 Iteration: 24460 | Train loss: -920.43719\n",
      "Epoch: 158/200 Iteration: 24480 | Train loss: -929.92615\n",
      "Epoch: 159/200 Iteration: 24500 | Train loss: -982.85834\n",
      "Epoch: 159/200 Iteration: 24520 | Train loss: -1202.72144\n",
      "Epoch: 159/200 Iteration: 24540 | Train loss: -1127.34204\n",
      "Epoch: 159/200 Iteration: 24560 | Train loss: -801.97766\n",
      "Epoch: 159/200 Iteration: 24580 | Train loss: -571.06696\n",
      "Epoch: 159/200 Iteration: 24600 | Train loss: -854.86267\n",
      "Epoch: 159/200 Iteration: 24620 | Train loss: -995.88300\n",
      "Epoch: 159/200 Iteration: 24640 | Train loss: -1127.16394\n",
      "Epoch: 160/200 Iteration: 24660 | Train loss: -919.91711\n",
      "Epoch: 160/200 Iteration: 24680 | Train loss: -969.28033\n",
      "Epoch: 160/200 Iteration: 24700 | Train loss: -1037.31140\n",
      "Epoch: 160/200 Iteration: 24720 | Train loss: -802.93689\n",
      "Epoch: 160/200 Iteration: 24740 | Train loss: -789.71484\n",
      "Epoch: 160/200 Iteration: 24760 | Train loss: -953.91876\n",
      "Epoch: 160/200 Iteration: 24780 | Train loss: -932.00232\n",
      "Epoch: 160/200 Iteration: 24800 | Train loss: -973.32336\n",
      "Epoch: 161/200 Iteration: 24820 | Train loss: -689.92126\n",
      "Epoch: 161/200 Iteration: 24840 | Train loss: -1204.35706\n",
      "Epoch: 161/200 Iteration: 24860 | Train loss: -798.24585\n",
      "Epoch: 161/200 Iteration: 24880 | Train loss: -821.07153\n",
      "Epoch: 161/200 Iteration: 24900 | Train loss: -1107.32422\n",
      "Epoch: 161/200 Iteration: 24920 | Train loss: -908.92944\n",
      "Epoch: 161/200 Iteration: 24940 | Train loss: -1102.76904\n",
      "Epoch: 162/200 Iteration: 24960 | Train loss: -1215.74097\n",
      "Epoch: 162/200 Iteration: 24980 | Train loss: -858.41235\n",
      "Epoch: 162/200 Iteration: 25000 | Train loss: -862.43646\n",
      "Epoch: 162/200 Iteration: 25020 | Train loss: -741.89142\n",
      "Epoch: 162/200 Iteration: 25040 | Train loss: -1084.98645\n",
      "Epoch: 162/200 Iteration: 25060 | Train loss: -653.12970\n",
      "Epoch: 162/200 Iteration: 25080 | Train loss: -962.47235\n",
      "Epoch: 162/200 Iteration: 25100 | Train loss: -976.81299\n",
      "Epoch: 163/200 Iteration: 25120 | Train loss: -983.91766\n",
      "Epoch: 163/200 Iteration: 25140 | Train loss: -1203.80518\n",
      "Epoch: 163/200 Iteration: 25160 | Train loss: -1144.72559\n",
      "Epoch: 163/200 Iteration: 25180 | Train loss: -885.06018\n",
      "Epoch: 163/200 Iteration: 25200 | Train loss: -593.99994\n",
      "Epoch: 163/200 Iteration: 25220 | Train loss: -846.56799\n",
      "Epoch: 163/200 Iteration: 25240 | Train loss: -1043.54456\n",
      "Epoch: 163/200 Iteration: 25260 | Train loss: -1140.28394\n",
      "Epoch: 164/200 Iteration: 25280 | Train loss: -940.13403\n",
      "Epoch: 164/200 Iteration: 25300 | Train loss: -960.55420\n",
      "Epoch: 164/200 Iteration: 25320 | Train loss: -1023.22217\n",
      "Epoch: 164/200 Iteration: 25340 | Train loss: -837.13892\n",
      "Epoch: 164/200 Iteration: 25360 | Train loss: -798.11969\n",
      "Epoch: 164/200 Iteration: 25380 | Train loss: -983.15961\n",
      "Epoch: 164/200 Iteration: 25400 | Train loss: -990.56799\n",
      "Epoch: 164/200 Iteration: 25420 | Train loss: -981.77808\n",
      "Epoch: 165/200 Iteration: 25440 | Train loss: -689.93866\n",
      "Epoch: 165/200 Iteration: 25460 | Train loss: -1237.82422\n",
      "Epoch: 165/200 Iteration: 25480 | Train loss: -786.56818\n",
      "Epoch: 165/200 Iteration: 25500 | Train loss: -819.07782\n",
      "Epoch: 165/200 Iteration: 25520 | Train loss: -1140.06921\n",
      "Epoch: 165/200 Iteration: 25540 | Train loss: -889.50903\n",
      "Epoch: 165/200 Iteration: 25560 | Train loss: -1153.50928\n",
      "Epoch: 166/200 Iteration: 25580 | Train loss: -1181.58862\n",
      "Epoch: 166/200 Iteration: 25600 | Train loss: -850.78888\n",
      "Epoch: 166/200 Iteration: 25620 | Train loss: -838.52789\n",
      "Epoch: 166/200 Iteration: 25640 | Train loss: -735.04303\n",
      "Epoch: 166/200 Iteration: 25660 | Train loss: -1099.49182\n",
      "Epoch: 166/200 Iteration: 25680 | Train loss: -664.57458\n",
      "Epoch: 166/200 Iteration: 25700 | Train loss: -953.96857\n",
      "Epoch: 166/200 Iteration: 25720 | Train loss: -951.80627\n",
      "Epoch: 167/200 Iteration: 25740 | Train loss: -1001.66486\n",
      "Epoch: 167/200 Iteration: 25760 | Train loss: -1227.09607\n",
      "Epoch: 167/200 Iteration: 25780 | Train loss: -1173.07764\n",
      "Epoch: 167/200 Iteration: 25800 | Train loss: -861.21796\n",
      "Epoch: 167/200 Iteration: 25820 | Train loss: -606.82697\n",
      "Epoch: 167/200 Iteration: 25840 | Train loss: -912.81897\n",
      "Epoch: 167/200 Iteration: 25860 | Train loss: -1067.14929\n",
      "Epoch: 167/200 Iteration: 25880 | Train loss: -1184.83496\n",
      "Epoch: 168/200 Iteration: 25900 | Train loss: -929.25342\n",
      "Epoch: 168/200 Iteration: 25920 | Train loss: -979.35468\n",
      "Epoch: 168/200 Iteration: 25940 | Train loss: -1074.72351\n",
      "Epoch: 168/200 Iteration: 25960 | Train loss: -870.58557\n",
      "Epoch: 168/200 Iteration: 25980 | Train loss: -835.56567\n",
      "Epoch: 168/200 Iteration: 26000 | Train loss: -976.81213\n",
      "Epoch: 168/200 Iteration: 26020 | Train loss: -994.78033\n",
      "Epoch: 168/200 Iteration: 26040 | Train loss: -1050.68787\n",
      "Epoch: 169/200 Iteration: 26060 | Train loss: -712.20795\n",
      "Epoch: 169/200 Iteration: 26080 | Train loss: -1240.93140\n",
      "Epoch: 169/200 Iteration: 26100 | Train loss: -826.95648\n",
      "Epoch: 169/200 Iteration: 26120 | Train loss: -893.89575\n",
      "Epoch: 169/200 Iteration: 26140 | Train loss: -1155.33826\n",
      "Epoch: 169/200 Iteration: 26160 | Train loss: -912.10370\n",
      "Epoch: 169/200 Iteration: 26180 | Train loss: -1184.56885\n",
      "Epoch: 170/200 Iteration: 26200 | Train loss: -1271.74756\n",
      "Epoch: 170/200 Iteration: 26220 | Train loss: -871.70374\n",
      "Epoch: 170/200 Iteration: 26240 | Train loss: -849.93951\n",
      "Epoch: 170/200 Iteration: 26260 | Train loss: -757.22321\n",
      "Epoch: 170/200 Iteration: 26280 | Train loss: -1136.50110\n",
      "Epoch: 170/200 Iteration: 26300 | Train loss: -670.38342\n",
      "Epoch: 170/200 Iteration: 26320 | Train loss: -1013.69641\n",
      "Epoch: 170/200 Iteration: 26340 | Train loss: -1034.50085\n",
      "Epoch: 171/200 Iteration: 26360 | Train loss: -1039.16553\n",
      "Epoch: 171/200 Iteration: 26380 | Train loss: -1264.86230\n",
      "Epoch: 171/200 Iteration: 26400 | Train loss: -1223.49939\n",
      "Epoch: 171/200 Iteration: 26420 | Train loss: -871.58313\n",
      "Epoch: 171/200 Iteration: 26440 | Train loss: -622.15564\n",
      "Epoch: 171/200 Iteration: 26460 | Train loss: -928.14209\n",
      "Epoch: 171/200 Iteration: 26480 | Train loss: -1055.56152\n",
      "Epoch: 171/200 Iteration: 26500 | Train loss: -1236.72095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172/200 Iteration: 26520 | Train loss: -981.77795\n",
      "Epoch: 172/200 Iteration: 26540 | Train loss: -1017.40845\n",
      "Epoch: 172/200 Iteration: 26560 | Train loss: -1088.70239\n",
      "Epoch: 172/200 Iteration: 26580 | Train loss: -872.45093\n",
      "Epoch: 172/200 Iteration: 26600 | Train loss: -857.97546\n",
      "Epoch: 172/200 Iteration: 26620 | Train loss: -1020.15625\n",
      "Epoch: 172/200 Iteration: 26640 | Train loss: -1006.73468\n",
      "Epoch: 172/200 Iteration: 26660 | Train loss: -1042.17236\n",
      "Epoch: 173/200 Iteration: 26680 | Train loss: -730.95612\n",
      "Epoch: 173/200 Iteration: 26700 | Train loss: -1293.93799\n",
      "Epoch: 173/200 Iteration: 26720 | Train loss: -849.49377\n",
      "Epoch: 173/200 Iteration: 26740 | Train loss: -878.72626\n",
      "Epoch: 173/200 Iteration: 26760 | Train loss: -1196.79419\n",
      "Epoch: 173/200 Iteration: 26780 | Train loss: -934.19232\n",
      "Epoch: 173/200 Iteration: 26800 | Train loss: -1171.27979\n",
      "Epoch: 174/200 Iteration: 26820 | Train loss: -1247.13818\n",
      "Epoch: 174/200 Iteration: 26840 | Train loss: -886.60437\n",
      "Epoch: 174/200 Iteration: 26860 | Train loss: -900.43390\n",
      "Epoch: 174/200 Iteration: 26880 | Train loss: -736.45227\n",
      "Epoch: 174/200 Iteration: 26900 | Train loss: -1223.62830\n",
      "Epoch: 174/200 Iteration: 26920 | Train loss: -683.02795\n",
      "Epoch: 174/200 Iteration: 26940 | Train loss: -1006.41248\n",
      "Epoch: 174/200 Iteration: 26960 | Train loss: -1041.01770\n",
      "Epoch: 175/200 Iteration: 26980 | Train loss: -1059.76343\n",
      "Epoch: 175/200 Iteration: 27000 | Train loss: -1299.46106\n",
      "Epoch: 175/200 Iteration: 27020 | Train loss: -1238.06897\n",
      "Epoch: 175/200 Iteration: 27040 | Train loss: -898.72955\n",
      "Epoch: 175/200 Iteration: 27060 | Train loss: -644.23419\n",
      "Epoch: 175/200 Iteration: 27080 | Train loss: -941.02991\n",
      "Epoch: 175/200 Iteration: 27100 | Train loss: -1085.14734\n",
      "Epoch: 175/200 Iteration: 27120 | Train loss: -1252.96899\n",
      "Epoch: 176/200 Iteration: 27140 | Train loss: -1006.53986\n",
      "Epoch: 176/200 Iteration: 27160 | Train loss: -1014.55029\n",
      "Epoch: 176/200 Iteration: 27180 | Train loss: -1128.93384\n",
      "Epoch: 176/200 Iteration: 27200 | Train loss: -918.47144\n",
      "Epoch: 176/200 Iteration: 27220 | Train loss: -851.75854\n",
      "Epoch: 176/200 Iteration: 27240 | Train loss: -1068.03662\n",
      "Epoch: 176/200 Iteration: 27260 | Train loss: -1070.95349\n",
      "Epoch: 176/200 Iteration: 27280 | Train loss: -1071.28162\n",
      "Epoch: 177/200 Iteration: 27300 | Train loss: -739.64197\n",
      "Epoch: 177/200 Iteration: 27320 | Train loss: -1294.83911\n",
      "Epoch: 177/200 Iteration: 27340 | Train loss: -879.78577\n",
      "Epoch: 177/200 Iteration: 27360 | Train loss: -906.08868\n",
      "Epoch: 177/200 Iteration: 27380 | Train loss: -1206.85168\n",
      "Epoch: 177/200 Iteration: 27400 | Train loss: -980.71643\n",
      "Epoch: 177/200 Iteration: 27420 | Train loss: -1234.22156\n",
      "Epoch: 178/200 Iteration: 27440 | Train loss: -1290.67688\n",
      "Epoch: 178/200 Iteration: 27460 | Train loss: -927.77734\n",
      "Epoch: 178/200 Iteration: 27480 | Train loss: -934.80780\n",
      "Epoch: 178/200 Iteration: 27500 | Train loss: -774.39014\n",
      "Epoch: 178/200 Iteration: 27520 | Train loss: -1173.77905\n",
      "Epoch: 178/200 Iteration: 27540 | Train loss: -717.22461\n",
      "Epoch: 178/200 Iteration: 27560 | Train loss: -1049.27942\n",
      "Epoch: 178/200 Iteration: 27580 | Train loss: -1031.08447\n",
      "Epoch: 179/200 Iteration: 27600 | Train loss: -1101.40405\n",
      "Epoch: 179/200 Iteration: 27620 | Train loss: -1359.49438\n",
      "Epoch: 179/200 Iteration: 27640 | Train loss: -1260.95874\n",
      "Epoch: 179/200 Iteration: 27660 | Train loss: -923.09119\n",
      "Epoch: 179/200 Iteration: 27680 | Train loss: -657.26862\n",
      "Epoch: 179/200 Iteration: 27700 | Train loss: -947.08484\n",
      "Epoch: 179/200 Iteration: 27720 | Train loss: -1155.25110\n",
      "Epoch: 179/200 Iteration: 27740 | Train loss: -1254.83533\n",
      "Epoch: 180/200 Iteration: 27760 | Train loss: -1045.94800\n",
      "Epoch: 180/200 Iteration: 27780 | Train loss: -1093.00378\n",
      "Epoch: 180/200 Iteration: 27800 | Train loss: -1101.37744\n",
      "Epoch: 180/200 Iteration: 27820 | Train loss: -919.86237\n",
      "Epoch: 180/200 Iteration: 27840 | Train loss: -881.91174\n",
      "Epoch: 180/200 Iteration: 27860 | Train loss: -1068.32947\n",
      "Epoch: 180/200 Iteration: 27880 | Train loss: -1067.03967\n",
      "Epoch: 180/200 Iteration: 27900 | Train loss: -1107.74463\n",
      "Epoch: 181/200 Iteration: 27920 | Train loss: -770.63159\n",
      "Epoch: 181/200 Iteration: 27940 | Train loss: -1357.68689\n",
      "Epoch: 181/200 Iteration: 27960 | Train loss: -921.94995\n",
      "Epoch: 181/200 Iteration: 27980 | Train loss: -911.60663\n",
      "Epoch: 181/200 Iteration: 28000 | Train loss: -1224.96753\n",
      "Epoch: 181/200 Iteration: 28020 | Train loss: -1000.09546\n",
      "Epoch: 181/200 Iteration: 28040 | Train loss: -1246.76965\n",
      "Epoch: 182/200 Iteration: 28060 | Train loss: -1284.86877\n",
      "Epoch: 182/200 Iteration: 28080 | Train loss: -953.88617\n",
      "Epoch: 182/200 Iteration: 28100 | Train loss: -919.14899\n",
      "Epoch: 182/200 Iteration: 28120 | Train loss: -798.61877\n",
      "Epoch: 182/200 Iteration: 28140 | Train loss: -1222.26624\n",
      "Epoch: 182/200 Iteration: 28160 | Train loss: -728.10291\n",
      "Epoch: 182/200 Iteration: 28180 | Train loss: -1101.49072\n",
      "Epoch: 182/200 Iteration: 28200 | Train loss: -1086.73462\n",
      "Epoch: 183/200 Iteration: 28220 | Train loss: -1089.61609\n",
      "Epoch: 183/200 Iteration: 28240 | Train loss: -1340.48877\n",
      "Epoch: 183/200 Iteration: 28260 | Train loss: -1287.39978\n",
      "Epoch: 183/200 Iteration: 28280 | Train loss: -941.28564\n",
      "Epoch: 183/200 Iteration: 28300 | Train loss: -658.60577\n",
      "Epoch: 183/200 Iteration: 28320 | Train loss: -979.42767\n",
      "Epoch: 183/200 Iteration: 28340 | Train loss: -1125.61780\n",
      "Epoch: 183/200 Iteration: 28360 | Train loss: -1302.95459\n",
      "Epoch: 184/200 Iteration: 28380 | Train loss: -1029.73645\n",
      "Epoch: 184/200 Iteration: 28400 | Train loss: -1061.88831\n",
      "Epoch: 184/200 Iteration: 28420 | Train loss: -1146.99231\n",
      "Epoch: 184/200 Iteration: 28440 | Train loss: -935.57373\n",
      "Epoch: 184/200 Iteration: 28460 | Train loss: -918.01892\n",
      "Epoch: 184/200 Iteration: 28480 | Train loss: -1081.10095\n",
      "Epoch: 184/200 Iteration: 28500 | Train loss: -1086.83740\n",
      "Epoch: 184/200 Iteration: 28520 | Train loss: -1163.51794\n",
      "Epoch: 185/200 Iteration: 28540 | Train loss: -789.76929\n",
      "Epoch: 185/200 Iteration: 28560 | Train loss: -1407.48096\n",
      "Epoch: 185/200 Iteration: 28580 | Train loss: -896.26392\n",
      "Epoch: 185/200 Iteration: 28600 | Train loss: -949.98236\n",
      "Epoch: 185/200 Iteration: 28620 | Train loss: -1288.93005\n",
      "Epoch: 185/200 Iteration: 28640 | Train loss: -1051.82031\n",
      "Epoch: 185/200 Iteration: 28660 | Train loss: -1260.27393\n",
      "Epoch: 186/200 Iteration: 28680 | Train loss: -1362.88989\n",
      "Epoch: 186/200 Iteration: 28700 | Train loss: -967.12006\n",
      "Epoch: 186/200 Iteration: 28720 | Train loss: -968.72406\n",
      "Epoch: 186/200 Iteration: 28740 | Train loss: -813.09033\n",
      "Epoch: 186/200 Iteration: 28760 | Train loss: -1233.76245\n",
      "Epoch: 186/200 Iteration: 28780 | Train loss: -735.72314\n",
      "Epoch: 186/200 Iteration: 28800 | Train loss: -1098.01599\n",
      "Epoch: 186/200 Iteration: 28820 | Train loss: -1091.36707\n",
      "Epoch: 187/200 Iteration: 28840 | Train loss: -1126.08276\n",
      "Epoch: 187/200 Iteration: 28860 | Train loss: -1379.83374\n",
      "Epoch: 187/200 Iteration: 28880 | Train loss: -1311.88281\n",
      "Epoch: 187/200 Iteration: 28900 | Train loss: -925.33545\n",
      "Epoch: 187/200 Iteration: 28920 | Train loss: -661.56445\n",
      "Epoch: 187/200 Iteration: 28940 | Train loss: -993.58154\n",
      "Epoch: 187/200 Iteration: 28960 | Train loss: -1205.05750\n",
      "Epoch: 187/200 Iteration: 28980 | Train loss: -1291.00806\n",
      "Epoch: 188/200 Iteration: 29000 | Train loss: -1053.14954\n",
      "Epoch: 188/200 Iteration: 29020 | Train loss: -1120.70215\n",
      "Epoch: 188/200 Iteration: 29040 | Train loss: -1183.60986\n",
      "Epoch: 188/200 Iteration: 29060 | Train loss: -966.15814\n",
      "Epoch: 188/200 Iteration: 29080 | Train loss: -926.56854\n",
      "Epoch: 188/200 Iteration: 29100 | Train loss: -1117.94482\n",
      "Epoch: 188/200 Iteration: 29120 | Train loss: -1146.22681\n",
      "Epoch: 188/200 Iteration: 29140 | Train loss: -1156.66016\n",
      "Epoch: 189/200 Iteration: 29160 | Train loss: -772.89642\n",
      "Epoch: 189/200 Iteration: 29180 | Train loss: -1372.24963\n",
      "Epoch: 189/200 Iteration: 29200 | Train loss: -902.88696\n",
      "Epoch: 189/200 Iteration: 29220 | Train loss: -951.36865\n",
      "Epoch: 189/200 Iteration: 29240 | Train loss: -1330.81982\n",
      "Epoch: 189/200 Iteration: 29260 | Train loss: -1041.75293\n",
      "Epoch: 189/200 Iteration: 29280 | Train loss: -1299.89258\n",
      "Epoch: 190/200 Iteration: 29300 | Train loss: -1407.67908\n",
      "Epoch: 190/200 Iteration: 29320 | Train loss: -967.67297\n",
      "Epoch: 190/200 Iteration: 29340 | Train loss: -967.92206\n",
      "Epoch: 190/200 Iteration: 29360 | Train loss: -804.64954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190/200 Iteration: 29380 | Train loss: -1274.02026\n",
      "Epoch: 190/200 Iteration: 29400 | Train loss: -763.46594\n",
      "Epoch: 190/200 Iteration: 29420 | Train loss: -1129.92249\n",
      "Epoch: 190/200 Iteration: 29440 | Train loss: -1123.41724\n",
      "Epoch: 191/200 Iteration: 29460 | Train loss: -1183.19385\n",
      "Epoch: 191/200 Iteration: 29480 | Train loss: -1459.72424\n",
      "Epoch: 191/200 Iteration: 29500 | Train loss: -1357.76636\n",
      "Epoch: 191/200 Iteration: 29520 | Train loss: -965.09253\n",
      "Epoch: 191/200 Iteration: 29540 | Train loss: -705.84253\n",
      "Epoch: 191/200 Iteration: 29560 | Train loss: -1030.45947\n",
      "Epoch: 191/200 Iteration: 29580 | Train loss: -1216.89465\n",
      "Epoch: 191/200 Iteration: 29600 | Train loss: -1349.62939\n",
      "Epoch: 192/200 Iteration: 29620 | Train loss: -1122.09351\n",
      "Epoch: 192/200 Iteration: 29640 | Train loss: -1131.31409\n",
      "Epoch: 192/200 Iteration: 29660 | Train loss: -1223.70239\n",
      "Epoch: 192/200 Iteration: 29680 | Train loss: -982.32898\n",
      "Epoch: 192/200 Iteration: 29700 | Train loss: -963.11804\n",
      "Epoch: 192/200 Iteration: 29720 | Train loss: -1142.72339\n",
      "Epoch: 192/200 Iteration: 29740 | Train loss: -1143.18860\n",
      "Epoch: 192/200 Iteration: 29760 | Train loss: -1160.83630\n",
      "Epoch: 193/200 Iteration: 29780 | Train loss: -826.53632\n",
      "Epoch: 193/200 Iteration: 29800 | Train loss: -1431.16516\n",
      "Epoch: 193/200 Iteration: 29820 | Train loss: -928.09839\n",
      "Epoch: 193/200 Iteration: 29840 | Train loss: -989.53955\n",
      "Epoch: 193/200 Iteration: 29860 | Train loss: -1348.83191\n",
      "Epoch: 193/200 Iteration: 29880 | Train loss: -1064.07458\n",
      "Epoch: 193/200 Iteration: 29900 | Train loss: -1329.27881\n",
      "Epoch: 194/200 Iteration: 29920 | Train loss: -1391.52307\n",
      "Epoch: 194/200 Iteration: 29940 | Train loss: -1007.61432\n",
      "Epoch: 194/200 Iteration: 29960 | Train loss: -1011.79028\n",
      "Epoch: 194/200 Iteration: 29980 | Train loss: -871.08826\n",
      "Epoch: 194/200 Iteration: 30000 | Train loss: -1260.67944\n",
      "Epoch: 194/200 Iteration: 30020 | Train loss: -785.38654\n",
      "Epoch: 194/200 Iteration: 30040 | Train loss: -1179.34509\n",
      "Epoch: 194/200 Iteration: 30060 | Train loss: -1151.91064\n",
      "Epoch: 195/200 Iteration: 30080 | Train loss: -1174.64417\n",
      "Epoch: 195/200 Iteration: 30100 | Train loss: -1454.48169\n",
      "Epoch: 195/200 Iteration: 30120 | Train loss: -1359.45923\n",
      "Epoch: 195/200 Iteration: 30140 | Train loss: -1003.11487\n",
      "Epoch: 195/200 Iteration: 30160 | Train loss: -708.68079\n",
      "Epoch: 195/200 Iteration: 30180 | Train loss: -1031.67615\n",
      "Epoch: 195/200 Iteration: 30200 | Train loss: -1223.89600\n",
      "Epoch: 195/200 Iteration: 30220 | Train loss: -1408.54651\n",
      "Epoch: 196/200 Iteration: 30240 | Train loss: -1139.26990\n",
      "Epoch: 196/200 Iteration: 30260 | Train loss: -1182.25940\n",
      "Epoch: 196/200 Iteration: 30280 | Train loss: -1232.92615\n",
      "Epoch: 196/200 Iteration: 30300 | Train loss: -1021.00732\n",
      "Epoch: 196/200 Iteration: 30320 | Train loss: -958.02423\n",
      "Epoch: 196/200 Iteration: 30340 | Train loss: -1153.00427\n",
      "Epoch: 196/200 Iteration: 30360 | Train loss: -1152.93921\n",
      "Epoch: 196/200 Iteration: 30380 | Train loss: -1255.92798\n",
      "Epoch: 197/200 Iteration: 30400 | Train loss: -829.86700\n",
      "Epoch: 197/200 Iteration: 30420 | Train loss: -1475.77783\n",
      "Epoch: 197/200 Iteration: 30440 | Train loss: -976.16748\n",
      "Epoch: 197/200 Iteration: 30460 | Train loss: -1037.04480\n",
      "Epoch: 197/200 Iteration: 30480 | Train loss: -1347.83911\n",
      "Epoch: 197/200 Iteration: 30500 | Train loss: -1092.63623\n",
      "Epoch: 197/200 Iteration: 30520 | Train loss: -1361.79932\n",
      "Epoch: 198/200 Iteration: 30540 | Train loss: -1399.66675\n",
      "Epoch: 198/200 Iteration: 30560 | Train loss: -998.22156\n",
      "Epoch: 198/200 Iteration: 30580 | Train loss: -1021.25958\n",
      "Epoch: 198/200 Iteration: 30600 | Train loss: -862.20422\n",
      "Epoch: 198/200 Iteration: 30620 | Train loss: -1325.85327\n",
      "Epoch: 198/200 Iteration: 30640 | Train loss: -798.61694\n",
      "Epoch: 198/200 Iteration: 30660 | Train loss: -1159.69141\n",
      "Epoch: 198/200 Iteration: 30680 | Train loss: -1163.66162\n",
      "Epoch: 199/200 Iteration: 30700 | Train loss: -1202.15588\n",
      "Epoch: 199/200 Iteration: 30720 | Train loss: -1462.42224\n",
      "Epoch: 199/200 Iteration: 30740 | Train loss: -1410.51965\n",
      "Epoch: 199/200 Iteration: 30760 | Train loss: -1028.32141\n",
      "Epoch: 199/200 Iteration: 30780 | Train loss: -744.52325\n",
      "Epoch: 199/200 Iteration: 30800 | Train loss: -1039.54309\n",
      "Epoch: 199/200 Iteration: 30820 | Train loss: -1286.82056\n",
      "Epoch: 199/200 Iteration: 30840 | Train loss: -1399.20471\n",
      "Epoch: 200/200 Iteration: 30860 | Train loss: -1128.13940\n",
      "Epoch: 200/200 Iteration: 30880 | Train loss: -1181.45215\n",
      "Epoch: 200/200 Iteration: 30900 | Train loss: -1254.94910\n",
      "Epoch: 200/200 Iteration: 30920 | Train loss: -1048.22241\n",
      "Epoch: 200/200 Iteration: 30940 | Train loss: -982.24182\n",
      "Epoch: 200/200 Iteration: 30960 | Train loss: -1224.47974\n",
      "Epoch: 200/200 Iteration: 30980 | Train loss: -1162.25671\n",
      "Epoch: 200/200 Iteration: 31000 | Train loss: -1206.35864\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/spooky-39.ckpt\n",
      "2148.0\n",
      "4000\n",
      "53.7\n"
     ]
    }
   ],
   "source": [
    "## Test: \n",
    "preds = rnn.predict(X_test)\n",
    "count = 0.0\n",
    "for i, guess in enumerate(preds):\n",
    "    if guess == y_test[i]:\n",
    "        count += 1.0\n",
    "        \n",
    "print count\n",
    "print len(preds)\n",
    "print (count/len(preds))*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/spooky-39.ckpt\n",
      "[1.5822611e-37 1.0000000e+00 2.3078283e-31 ... 1.0000000e+00 1.0000000e+00\n",
      " 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "## Get probabilities:\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4079\n"
     ]
    }
   ],
   "source": [
    "print len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "print len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
